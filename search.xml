<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[tensorflow二次开发]]></title>
    <url>%2F2019%2F02%2F27%2Ftensorflow%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[编译 方法1：./configurebazel build –config=opt //tensorflow/tools/pip_package:build_pip_package build出错清理：/root/.cache/bazel把下面的之前出错的缓存文件给删除掉 生成whell包bazel-bin/tensorflow/tools/pip_package/build_pip_package /root/tensorflow/wheel_pkg/build_withSource 方法2：yes “” | python configure.pybazel build –config=mkl –copt=-mavx2 –copt=-O3 –copt=-DINTEL_MKL_QUANTIZED -s //tensorflow/tools/pip_package:build_pip_package 生成whell包bazel-bin/tensorflow/tools/pip_package/build_pip_package /root/tensorflow/wheel_pkg/build_withSource 增量编译直接bazel build然后重新生成wheel包 pip unistall tensorflow一定先卸载然后重新安装否则还是原来的包 编译之后生成pywrap_tensorflow_internal.py 以及 pywrap_tensorflow_internal.cc在~/.cache/bazel目录下面,所有代码都在_pywrap_tensorflow_internal.so 的动态链接库里面pywrap_tensorflow_internal.py: 负责对接上层 Python 调用pywrap_tensorflow_internal.cc: 负责对接下层 C API 调用 pywrap_tensorflow_internal.py 模块首次被导入时，自动地加载 _pywrap_tensorflow_internal.so 的动态链接库；其中， _pywrap_tensorflow_internal.so包含了整个 TensorFlow 运行时的所有符号。 在 pywrap_tensorflow_internal.cc 的实现中，静态注册了一个函数符号表，实现了 Python 函数名到 C 函数名的二元关系。在运行时，按照 Python 的函数名称，匹找到对应的 C 函数实现，最终实现 Python 到 c_api.c 具体实现的调用关系。 编译debug版本的tensorflowgdb 调试二种方法方法去debug TF:method1: gdb python run file.py bt method2: 跑测试 top 看到python进程的pid gdb -p pid挂上之后，原来测试会挂住bt 函数名或者其它打上断点continue 继续测试直到core-dump 看python到C++调用关系以Session 为例子：tf.Session时候的调用关系 python api/root/tensorflow_src/test_code/private-tensorflow/tensorflow/python目录下面： grep -rni “class Session”client/session.py:1475:class Session(BaseSession):里面调用了baseSession的构造函数 看baseSession里面调用了tf_session 12self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)from tensorflow.python import pywrap_tensorflow as tf_session 看pywrap_tensorflow.py这个就是对应了编译出来的so文件 在source insight里面搜索TF_NewSessionRef看到定义在tf_session_help.cc里面里面调用了TF_NewSession source insight里面搜索TF_NewSession已经进入到C++ 代码内部 自己定义个operation]]></content>
      <tags>
        <tag>技术 tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建电商搜索引擎]]></title>
    <url>%2F2019%2F02%2F08%2F%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[介绍做的一个电商的搜索引擎使用elk作为后台架构，elastic提供搜索的接口，logstash作为mysql和elastic之间数据同步的桥梁 其实这个只是常用引擎的第一步，一般的搜索引擎为了提高用户的点击率，在得到所有相关的物品之后，会进一步输入到一个模型当中，包含了用户的使用行为等参数，最后根据用户可能的购买率进行排序。参考wide&amp;deep模型：https://zhuanlan.zhihu.com/p/29640272 logstash配置logstash的基本安装和配置可以参考logstash的文章如何做数据同步参考这边文章https://zhuanlan.zhihu.com/p/40177683 logstash 每次只同步更新一条数据的问题https://stackoverflow.com/questions/41418060/logstash-is-indexing-only-one-row-of-select-query-from-mysql-to-elastic-search 配置的参数的官方文档：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html 完整的logstash的配置初始化时候的配置,logstash-es-mysql.conf1234567891011121314151617181920212223242526272829303132333435363738input&#123; stdin &#123; &#125; jdbc &#123; jdbc_driver_library =&gt; &quot;/home/elastic/mysql-connector/mysql-connector-java.jar&quot; jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_connection_string =&gt; &quot;jdbc:mysql://116.62.126.101:3306/manzuoTestENV?characterEncoding=utf8&amp;useSSL=false&quot; jdbc_user =&gt; &quot;manzuo&quot; jdbc_password =&gt; &quot;password&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50000&quot; schedule =&gt; &quot;* * * * * &quot;#配置为每一分钟都更新一次 type =&gt; &quot;jdbc&quot; statement =&gt; &quot;select * from `index_alllist`&quot; #statement =&gt; &quot;select itemId,title,title2,parameter,author from `index_alllist`&quot; tracking_column =&gt; &quot;itemId&quot; tracking_column_type =&gt; &quot;numeric&quot; clean_run =&gt; true &#125;&#125;filter&#123; json&#123; source =&gt; &quot;message&quot; remove_field =&gt; [&quot;message&quot;] &#125;&#125;output&#123; elasticsearch &#123; hosts =&gt; [&quot;http://47.75.156.162:9200&quot;] index =&gt; &quot;manzuo_list&quot; document_type =&gt;&quot;items&quot; document_id =&gt; &quot;%&#123;itemid&#125;&quot; #这里全用小写的itemid，很重要，和mysql的大小写不同，应为es字段全是小写的啦 &#125; stdout &#123; codec =&gt; json_lines &#125;&#125; 启动logstash:./bin/logstash -f logstash-es-mysql.conf 启动之前首先创建索引，因为要使用中文的分词器先安装中文分词器的插件https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-smartcn.html删除已有的索引(存在的话):DELETE /manzuo_list设置新索引的analyzer:1234567891011121314PUT /manzuo_list&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;default&quot;: &#123; &quot;type&quot;: &quot;smartcn&quot; &#125; &#125; &#125; &#125; &#125;&#125; 启动logstash去同步数据:./bin/logstash -f logstash-es-mysql.conf搜索:12345678GET /manzuo_list/items/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;核桃&quot; &#125; &#125;&#125; 参考中文教程https://www.elastic.co/cn/blog/how-to-search-ch-jp-kr-part-1 存在的问题这样的配置只能做到mysql中的新数据向es中去追加一旦mysql删除了老数据，es并不会同步去删除]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe中的生产者模式]]></title>
    <url>%2F2019%2F01%2F31%2F%E6%B3%A8%E5%86%8C%E5%B1%82%2F</url>
    <content type="text"><![CDATA[在这个代码layer_factory.hpp的开头作者注释了怎么注册layer 比如我们PB的模型文件中有一层的type是DummyData 通过搜索grep -rni “REGISTER_LAYER_CLASS(Du” ./有些层搜不到比如Convolution那就可以搜索grep -rni “INSTANTIATE_CLASS(Convolution” ./ ./src/caffe/layers/dummy_data_layer.cpp:149:REGISTER_LAYER_CLASS(DummyData);可以搜索到在哪里注册层 从而在后面代码里面调用对应层的forward函数的时候就找到对应的实现了 注意 有一个很trick的地方比如我在PB文件里定义了一个层的type为InnerProduct按照上面的说法我们搜索：[root@localhost caffe-1.1.3]# grep -rni “REGISTER_LAYER_CLASS(InnerProduct” ././src/caffe/layers/inner_product_layer.cpp:189:// REGISTER_LAYER_CLASS(InnerProduct);[root@localhost caffe-1.1.3]# grep -rni “INSTANTIATE_CLASS(InnerProduct” ././src/caffe/layers/inner_product_layer.cpp:187:INSTANTIATE_CLASS(InnerProductLayer); 认为这个层定义在./src/caffe/layers/inner_product_layer.cpp这个文件里面，看对应的forward_cpu函数但其实不是因为我们默认或者指定使用了mkldnn作为engine在layer_factory.cpp的下面有一个函数GetInnerProductLayer里面会根据engine去找对应的层return shared_ptr]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MKLDNN与AVX512]]></title>
    <url>%2F2019%2F01%2F19%2FMKLDNN%E4%B8%8EAVX512%2F</url>
    <content type="text"><![CDATA[解释以caffe2为例子进行解释：caffe2的输入的数据格式为NCHWbatchsize，channel，height和width 对于caffe2来说，在每一层卷积的时候就会直接把这个NCHW的数据塞进mkldnn对应的primitive里面进行计算， mkldnn的primitive并行化计算包含两个层面，thread的层面和单个thread内部 NH方向的并行化对于NCHW的数据格式来说, N和H会对应了threads层面的并行化计算，也就是batchsize张图片或者一张图片很大也会被分配到多个不同的threads(每个thread可能绑定了CPU计算的core上)这解释了caffe2，detecture的时候batchsize从1增加到8，性能增加不明显的问题因为detecture的图片像素点很大10241024,正常resnet50的输入都是224224，所以detecture的图片在H方向上已经在多threads上并行计算了，增大batchsize无法提高性能 为什么说c16（channel数是16的倍数）很重要channel数是16的倍数，我们经常看到卷积核的输出的channel经常是32,64等16的倍数 AVX512AVX512指令集的意思就是一次处理512bit的数据，一个FP32的数据占了32bit，所以一次性可以处理16个FP32的数据 对应了上面的卷积计算来说，假设有个NCHW的数据，一个batchsize有32个通道，那我们定义的卷积核也有32的通道在第一个像素点上的卷积，可以先读取16个channel(通道)的第一个像素点的数据进来，对应了前16个卷积核的第一个像素点数据，进行一次卷积运算 同时，一个物理核(这里不确定是物理核还是逻辑核)会有 16？？ 个AVX512可以读取的寄存器，每个寄存器可以存512bit，所以一次性可以读每个通道的前16个像素点进来，一共(16(通道数)32(一个浮点数)16(一组16个像素点组成的浮点数))一起存在16个AVX512的数据的寄存器里面]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql备份]]></title>
    <url>%2F2019%2F01%2F18%2Fmysql%E5%A4%87%E4%BB%BD%2F</url>
    <content type="text"><![CDATA[冷备份生成sql文件mysqldump -u用户名 -p 数据库名 &gt; 数据库名.sql范例：mysqldump -umanzuo -p -h116.62.126.101 manzuoTestENV &gt; manzuoTestENV.sql 导入sql文件mysql -u用户名 -p 数据库名 &lt; 数据库名.sql范例：mysql -uroot -p manzuoTestENV &lt; manzuoTestENV.sql]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcpdump]]></title>
    <url>%2F2019%2F01%2F08%2Ftcpdump%2F</url>
    <content type="text"><![CDATA[介绍tcpdump可以用来抓包，在微服务架构下可以用来检查服务发现和rpc调用的包是否发送成功或者接收成功 用法入门教程]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决github冲突]]></title>
    <url>%2F2018%2F12%2F31%2F%E8%A7%A3%E5%86%B3github%E5%86%B2%E7%AA%81%2F</url>
    <content type="text"><![CDATA[工具sublime Merge 流程 stashgit stashgit pullgit stash pop commitgit add .git commit -mgit pullgit commit append]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[定位进程占用内存问题]]></title>
    <url>%2F2018%2F11%2F01%2F%E5%AE%9A%E4%BD%8D%E8%BF%9B%E7%A8%8B%E5%8D%A0%E7%94%A8%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[定位一个服务内存占用率过高的问题： 起因机器报警，内存不足，用free命令去看，看到内存不足用top 然后M按内存使用率排序，找到服务的进程号top -p 进程号看到这个进程占用了1.5G的虚拟内存 分析12345678910111213141516cat /proc/&lt;pidnum&gt;/status......Groups: 0 1 2 3 4 6 10VmPeak: 1614960 kBVmSize: 1549424 kBVmLck: 12 kBVmPin: 0 kBVmHWM: 65352 kBVmRSS: 33520 kBVmData: 1046480 kBVmStk: 688 kBVmExe: 640 kBVmLib: 55276 kBVmPTE: 540 kBVmSwap: 0 kBThreads: 30 VmStk 栈区大概688KB还ok看到VmData占了将近1G大小，google了一下VmData介绍说是堆区 把代码遍历了一遍，找到所有new的地方，看看有没有delete，排除了内存泄漏的情况然后在代码中print了一把所有的new的类 cout&lt;&lt;sizeof(classname)&lt;&lt;endl;看到这些类加起来也就100KB，一共100个微线程加起来也就10MB的样子 进一步分析这时候就纠结了问了几个老同事，提供了些新的思路： 内存泄漏，已经排除过，而且程序刚启动就占用了很大的内存大小 把微线程限制到1个，在程序运行过程中，打印内存使用情况 解决这时候有个以前搞安全的同事帮忙一起看了：cat /proc/29912/smaps看看https://blog.csdn.net/tangtang_yue/article/details/78298067smaps文件中，每一条记录表示进程虚拟内存空间中一块连续的区域其中权限标识，rw-s表示可读可写的共享内存，rw-p表示可读可写的私有还可以在里面搜索heap和stack，应该有多块连续区域，可以累加得到堆栈的大小进一步在这个文件里搜索：看到最大的一块区域：5249 7fb495e28000-7fb49c228000 rw-s 00000000 00:04 1277991 /SYSV10910931 (deleted)5250 Size: 102400 kB /SYSV10910931 表示是一个共享内存 在/proc目录下,grep -rni “SYSV10910931” ./*/smaps 看到所有使用这块共享内存的进程 然后一个个单独 top -p 进程 判断一下哪个进程是最后可能创建这个共享内存的]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一次刷数据的经历]]></title>
    <url>%2F2018%2F10%2F29%2F%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E5%88%B7%E6%95%B0%E6%8D%AE%E7%9A%84%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[背景今天刷线上数据，需求很简单，DB中有一个字段，需要向缓存中添加。具体的做法就是，将DB中updatetime字段加1更新一下，从而可以触发同步组件，从DB拉取数据向CKV同步。 过程看似简单的一个操作，在测试环境实验了多遍之后，第一次在线上环境操作的时候还是踩坑了看到的现象: 在刷完数据之后，用脚本对比数据发现最后三条数据没有从db向CKV更新。解决方法: 还好数据量不大，就手动把这三条数据刷了一把。但是还是要找原因: 原因是在我刷数据的过程中，有人向数据库新添加了三条数据(亏我还选在了晚上来刷)。按照同步组件的原理，这新添加的三条数据应该已经往CKV中同步了。但是，因为我是先获取数据库中数据量，然后通过主键去遍历更新数据库中的每一条数据。而新添加的这三条数据，根据主键排序在数据库的中间位置，极小概率会出现在数据库的末尾，所以导致原来在数据库的末尾的三条数据超出了更新的范围(更新的范围在初始化时count一把数据库的值)。根本的解决办法: 从第一次的刷新的结束点开始往末尾再刷一下数据 全量再刷一遍数据 刷数据的注意点 运行脚本的时候，bash test.sh | tee run.log 2 &amp; 一定要要全量保存日志，方便出错定位 一定要写个脚本在刷完数据之后，全量比较一下DB和缓存中的数据是否一致，确保全量更新成功]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[介绍微服务架构应该是近年来，软件应用领域非常热门的概念微服务的概念源于2014年3月Martin Fowler所写的一篇文章“Microservices” 原有架构的缺点 只改变一部分代码，需要重新构建并部署整个项目 负载均衡与扩容，需要将整个项目一起横向扩容 微服务架构的优点 改变部分代码，只需要将单个微服务构建，可以方便的做到敏捷上线 负载均衡与扩容，单个微服务负载过高，只需要将单个微服务扩容 每个开发者只需要负责某几个微服务模块，可以显著提高开发效率 只要规定好微服务模块之间的通信协议，比如使用protobuf，就可以用不同语言编译协议，这也就允许不同模块之间采用不同语言开发 关键技术点 微服务之间通信，协议的定义非常重要，可以使用protobuf 单个微服务的并发能力的提高，微服务之间需要通过tcp和udp协议进行通信，这就意味在发送数据和等待接受数据之间，这个进程是hang的状态。所以微服务架构需要使用多进程，多线程，多协程(进程和线程的调度都是内核态，而协程的调度可以由框架实现，在用户态，这大大减小了协程之间切换的效率)。腾讯内部的spp框架和开源的msec框架就采用了类似的概念。]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序开发]]></title>
    <url>%2F2018%2F10%2F13%2F%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[框架前端：小程序后台： django数据库: mysql缓存: redis 数据库搭建配置mysql支持中文https://blog.csdn.net/u014657752/article/details/48206885https://blog.csdn.net/crave_shy/article/details/23345869再修改一个databases的字符集:https://stackoverflow.com/questions/22572558/how-to-set-character-set-database-and-collation-database-to-utf8-in-my-ini重启mysql，/etc/init.d/mysql restart 创建新用户创建数据库，并授权给新用户开启数据库远程登录https://blog.csdn.net/xiexievv/article/details/50513996授权然后注释：#bind-address = 127.0.0.1vim /etc/mysql/mysql.conf.d/mysqld.cnf 重启/etc/init.d/mysql restart django初始化时python manage.py migrate之后某个APP改动之后$ python manage.py makemigrations TestModel # 让 Django 知道我们在我们的模型有一些变更$ python manage.py migrate TestModel # 创建表结构]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rapidJson]]></title>
    <url>%2F2018%2F09%2F22%2FrapidJson%2F</url>
    <content type="text"><![CDATA[简介RapidJSON 是一个高效的 C++ JSON 解析／生成器http://rapidjson.org/zh-cn/md_doc_tutorial_8zh-cn.html 序列化12345678rapidjson::StringBuffer buffer1;rapidjson::Writer&lt;rapidjson::StringBuffer&gt; writer1(buffer1); writer1.StartObject();writer1.Key(&quot;count&quot;);writer1.Int(2); writer1.EndObject();printf(&quot;%s\n&quot;, buffer1.GetString()); 反序列化12345678910111213141516171819#include &quot;rapidjson/document.h&quot;using namespace rapidjson;string a=&quot;&#123; &quot;hello&quot;: &quot;world&quot;, &quot;t&quot;: true , &quot;f&quot;: false, &quot;n&quot;: null, &quot;i&quot;: 123, &quot;pi&quot;: 3.1416, &quot;a&quot;: [1, 2, 3, 4]&#125;&quot;Document document;document.Parse(a.c_str());if(!document.HasMember(&quot;hello&quot;))&#123; return -1;&#125;printf(&quot;hello = %s\n&quot;, document[&quot;hello&quot;].GetString());]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[protobuf]]></title>
    <url>%2F2018%2F09%2F01%2Fprotobuf%2F</url>
    <content type="text"><![CDATA[安装下载进入安装教程进入release版本目录下载C++版本release安装过程参考 12345./autogen.sh./configuremakemake checkmake install 教程参考文档]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[svn]]></title>
    <url>%2F2018%2F08%2F30%2Fsvn%2F</url>
    <content type="text"><![CDATA[一般先在服务器上建立个仓库12mkdir /root/CgiNgsvnadmin create /root/CgiNg 进入/root/CgiNg修改conf相关配置文件一般相关的有三个配置文件authz：负责账号权限的管理，控制账号是否有读写权限passwd：负责账号和密码的用户名单管理svnserve.conf：svn服务器配置文件具体修改哪些内容，可以参考这个链接 启动服务1svnserve -d -r /root/CgiNg 下载仓库windows上用tortoise svn右键checkout，输入地址svn://149.28.149.94:3690，和下载到本地的地址Linux上，12mkdir repo &amp;&amp; cd reposvn co svn://149.28.149.94:3690 提交文件 所有文件12svn add ./*svn commit -m &quot;message&quot; 注意在服务器端仓库中文件是用另外格式存储的，所以你在服务器端find是找不到代码文件的 单独文件 12svn add 文件名svn commit -m &quot;message&quot; 目录以及目录下文件 12svn add 目录svn commit -m &quot;message&quot; 删除文件(注意删除本地和远程的)两种方法： 方法1直接删除远程文件svn delete svn://路径(目录或文件的全路径) -m “删除备注信息文本” 推荐如下操作： 先删除本地文件然后commit上去svn delete 文件名svn ci -m “删除备注信息文本” &lt;- ci是commit的简写 不小心add了多余的文件1234svn add ./*之后发现有的文件不应该提交svn revert 文件svn revert --depth infinity 目录 Linux端命令详细手册]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言处理参数不定函数]]></title>
    <url>%2F2018%2F08%2F09%2FC%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%8F%82%E6%95%B0%E4%B8%8D%E5%AE%9A%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[使用va函数示例：求N个数的和 int sum(int count, …){ int sum = 0; int i; va_list ap; va_start(ap, count); for (i = 0; i &lt; count; ++i) { sum += va_arg(ap, int); } va_end(ap); return sum;} 下面是 里面重要的几个宏定义如下：typedef char* va_list;void va_start ( va_list ap, prev_param );type va_arg ( va_list ap, type );void va_end ( va_list ap );va_list 是一个字符指针，可以理解为指向当前参数的一个指针，取参必须通过这个指针进行。 在调用参数表之前，定义一个 va_list 类型的变量，(假设va_list 类型变量被定义为ap)； 然后应该对ap 进行初始化，让它指向可变参数表里面的第一个参数，这是通过 va_start 来实现的，第一个参数是 ap 本身，第二个参数是在变参表前面紧挨着的一个变量,即“…”之前的那个参数； 然后是获取参数，调用va_arg，它的第一个参数是ap，第二个参数是要获取的参数的指定类型，然后返回这个指定类型的值，并且把 ap 的位置指向变参表的下一个变量位置； 获取所有的参数之后，我们有必要将这个 ap 指针关掉，以免发生危险，方法是调用 va_end，他是输入的参数 ap 置为 NULL，应该养成获取完参数表之后关闭指针的习惯。说白了，就是让我们的程序具有健壮性。通常va_start和va_end是成对出现。http://www.cnblogs.com/hanyonglu/archive/2011/05/07/2039916.htmlhttps://blog.csdn.net/xyang81/article/details/41223527]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logstash]]></title>
    <url>%2F2018%2F08%2F05%2Flogstash%2F</url>
    <content type="text"><![CDATA[安装配置在配置文件logstash.conf中修改input指定log文件的位置filter对日志进行处理和格式化output指定输出到redis或者elastic 配置文件中一定需要显示指定一个input和output启动logstashbin/logstash -f logstash.conf 报错Logstash could not be started because there is already another instance using the configured data directory解决./bin/logstash -f test.conf –path.data=/home/elastic filter 介绍date filter的作用，原来输出字段中的@timestamp是读取系统时间，通过配置filter date可以从日志的message中简析出时间并格式化时间到@timestamp字段当中先用grok filter从mesage里面解析并转换生成新的字段 看看grok的用法，基本表达式%{SYNTAX:SEMANTIC}SYNTAX是指定的一个正则表达式，上面这句话的意思就是从message字段中，将匹配到SYNTAX的内容放到新生成的SEMANTIC字段当中 所以学好logstash一定要熟悉grok的正则匹配 配置模板 监听ssh用户登录input{ file{ path =&gt; “/var/log/secure” }}output{ stdout{}} 输出到elasticsearchinput{ file{ path =&gt; “/var/log/secure” }}output{ elasticsearch{ hosts =&gt; “http://10.239.182.93:9200“ index =&gt; “logstash-test” document_type =&gt; “test” }} 简析系统日志elasticsearch配置input{ file{ path =&gt; “/var/log/secure” type =&gt; “system-ssh-log” } }filter{ grok{ match=&gt;{“message”=&gt;”%{SYSLOGBASE}”} }}output{ elasticsearch{ hosts =&gt; “http://10.239.182.93:9200“ index =&gt; “logstash-%{type}-%{+YYYY.MM.dd}” document_type =&gt; “%{type}” } } SYSLOGBASE是很重要的一个系统日志匹配的正则表达式]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic]]></title>
    <url>%2F2018%2F07%2F31%2Felastic%2F</url>
    <content type="text"><![CDATA[安装创建非root用户adduser elasticpasswd elastic 使用新用户下载curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.10.tar.gz 启动：./bin/elasticsearch 打开另一个终端进行测试： curl ‘http://localhost:9200/?pretty‘你能看到以下返回信息：{ “status”: 200, “name”: “Shrunken Bones”, “version”: { “number”: “1.4.0”, “lucene_version”: “4.10” }, “tagline”: “You Know, for Search”} 报错：[root@localhost ~]# curl -XGET ‘http://localhost:9200/?pretty‘ Redirection Redirect 关闭防火墙和代理unset $http_proxyunset $https_proxy 修改绑定ip尝试修改配置文件 elasticsearch.yml中network.host(注意配置文件格式不是以#开头的要空一格， ：后要空一格) 为network.host: 0.0.0.0 安装kibana sense下载解压kibana5.6.10Sense was renamed to Console and is built into Kibana 5.https://github.com/elastic/sense/blob/master/README.md 直接启动./bin/kibana会连接上默认配置的elastic restful API命令格式命令格式：https://www.elastic.co/guide/cn/elasticsearch/guide/current/_talking_to_elasticsearch.html -i 选项curl -i -XGET ‘localhost:9200/‘-i指定请求返还响应的头 简单搜索GET /megacorp/employee/_search?q=last_name:Smith直接在url后边添加参数，但是这种方式不支持单词的部分匹配GET /megacorp/employee/_search?q=last_name:Sm是搜索不到Smith的 指定请求体count返回符合条件的文档数量curl -XGET localhost:9200/_count?pretty -d ‘{ “query”: { “match_all”: {} }}’ count返回符合条件的文档数量curl -XGET ‘localhost:9200/megacorp/employee/_search’ -d ‘{ “query” : { “match” : { “last_name” : “Smith” } }}’ 单查询与组合多查询 单查询‘{ “query” : { &quot;match&quot; : { &quot;last_name&quot; : &quot;Smith&quot; } }}’query和filter的区别filter用于精确字段的精确查找和过滤，没有scoce，只返回精确匹配查找到的docquery用于 filtered用于组合query和filter‘{“query”:{ “filtered”:{ &quot;query&quot; : { &quot;match&quot; : { &quot;last_name&quot; : &quot;Smith&quot; } }, &quot;filter&quot;:{ term:{ &quot;age&quot;:12 } } } }}’组合多查询:使用boolbool下面可以包含4个子句: must,must_not,should,filter‘{ “query” : { &quot;bool&quot;: { &quot;must&quot;: { &quot;match&quot; : { &quot;last_name&quot; : &quot;smith&quot; } }, &quot;filter&quot;: { &quot;range&quot; : { &quot;age&quot; : { &quot;gt&quot; : 30 } } } } }}’constant_score可以用来代替bool查询语句https://www.elastic.co/guide/cn/elasticsearch/guide/current/combining-queries-together.html 创建索引或者添加文档添加文档时，若索引不存在会自动创建索引，可以指定自动创建的索引所使用的模板，这样对于自动创建的索引进行控制curl -XPUT ‘localhost:9200/intel/employee/3’ -d ‘{ “first_name” : “Douglas”, “last_name” : “Fir”, “age” : 35, “about”: “I like to build cabinets”, “interests”: [ “forestry” ]}’创建索引时有三个配置项，shards,replication,以及analysisanalysis的使用：https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html定制化分析器：可以配置type为某个内置的analyzer然后配置这个analyzer的选项，也可以直接指定type为custom，然后配置tokenizer，filter以及char_filterhttps://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-custom-analyzer.html 关闭和打开索引http://cwiki.apachecn.org/pages/viewpage.action?pageId=4882797curl -XPOST ‘localhost:9200/my_index/_close?pretty’curl -XPOST ‘localhost:9200/my_index/_open?pretty’ 查询语句的区别:https://www.elastic.co/guide/cn/elasticsearch/guide/current/_most_important_queries.htmlmatch: 会用对应查询的字段的分析器去分词并匹配计算score，与term的区别match_all: 匹配所有对应文档 { “query”: { “match_all”: {} } }match_phrase: 精确完全匹配上整串短语,保留那些包含 全部 搜索词项，且 位置 与搜索词项相同的文档https://www.elastic.co/guide/cn/elasticsearch/guide/current/_phrase_search.htmlterm:查询被用于精确值 匹配,注意term区分大小写，所以一个字段field如果是text类型，在倒排索引中会被状态成小写，直接term查找会找不到，需要term查找这个字段的field.keywordterms:terms 查询和 term 查询一样，但它允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件prefix: 前缀查询，不会对输入的查询信息进行analyzewildcard:使用标准的 shell 通配符查询regexp:使用正则表达式通配符查询match_phrase_prefix:range：multi_match: 在多个字段上搜索同一个文本https://www.elastic.co/guide/cn/elasticsearch/guide/current/multi-match-query.html有三种评分方式：best_field,most_field,cross_fieldhttps://www.elastic.co/guide/cn/elasticsearch/guide/current/_cross_fields_queries.html scroll：游标查询，可以用于批量查询数据https://www.elastic.co/guide/cn/elasticsearch/guide/current/scroll.htmlfuzzy:模糊匹配https://www.elastic.co/guide/cn/elasticsearch/guide/current/fuzzy-query.htmlrange和term常用于filter 查看字段映射信息GET /_mapping?prettyGET /index/_mapping?prettyGET /index/_mapping/type?pretty mapping中text和keyword的区别“about”: { “type”: “text”, “fields”: { “keyword”: { “type”: “keyword”, “ignore_above”: 256 } }}about字段类型是text，默认会被analyze和倒排索引about.keyword字段则不会被analyze “fielddata”: truefielddata主要用于聚类，只有text字段要配置fielddata用于倒排索引，text字段的fielddata默认是关闭的，需要对字段聚类时要配置它打开:https://blog.csdn.net/baristas/article/details/78974090其它类型的字段用doc_values用于倒排索引 doc_values与fielddata的区别doc_values用于非text字段聚合，默认是打开的，作为倒排索引的转置存放在硬盘上，可以在创建索引时关闭fielddata用于text字段，默认是关闭的，可以创建索引时打开，第一次使用字段的聚类时生成，并存放在内存中 查看分析器curl -XGET http://10.239.182.93:9200/_analyze?pretty -d ‘{“analyzer”:”standard”,”text”:”Text to analyze”}’用于查看standard分析器是如何分析Text to analyze这句话的 获得具体搜索时分析的信息GET /my_index/my_type/_validate/query?explain{ “query”: { “match”: { “name”: “brown fo” } }}GET /my_index/doc/_search?explain{ “query”: { “term”: { “text”: “fox” } }} search的时候评分的规则https://www.elastic.co/guide/cn/elasticsearch/guide/current/scoring-theory.html 排序：sort 聚类理解两个概念，桶和度量分桶之后，根据指定的度量计算每个桶的度量值 地理位置地理坐标点不能被动态映射 （dynamic mapping）自动检测，而是需要显式声明对应字段类型为 geo-point123456789101112131415PUT /attractions&#123; &quot;mappings&quot;: &#123; &quot;restaurant&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125; &#125; &#125; &#125;&#125; 集群状态监测GET _cluster/healthGET _nodes/statsGET _cluster/statshttps://www.elastic.co/guide/cn/elasticsearch/guide/current/_cluster_stats.htmlGET /_cat 列出所有可用的cat APIGET /_cat/healthGET /_cat/health?v?v 参数： 显示表头 #集群修改vim config/elasticsearch.ymlcluster.name: leslie-elastic #多个instance同一个cluster namenode.name: node-1 #独立的node namenetwork.host: 10.239.182.240 #外网访问的IPdiscovery.zen.ping.unicast.hosts: [“10.239.182.240”, “10.239.182.51”]默认是单播模式，只会在一台机器上去搜索同一个名字的elastic，希望搜索其它host需要在discovery.zen.ping.unicast.hosts添加上搜索的IP vim config/kibana.ymlserver.host: 10.239.182.240elasticsearch.url: “http://10.239.182.240:9200“]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反汇编]]></title>
    <url>%2F2018%2F05%2F21%2F%E5%8F%8D%E6%B1%87%E7%BC%96%2F</url>
    <content type="text"><![CDATA[准备编译后的目标文件中包含哪些内容：https://www.jianshu.com/p/4c938ccda653 objdump用于将机器码生成汇编代码gcc命令objdump用法objdump -x obj 以某种分类信息的形式把目标文件的数据组织（被分为几大块）输出 &lt;可查到该文件的所有动态库&gt;objdump -t obj 输出目标文件的符号表()objdump -h obj 输出目标文件的所有段概括()objdump -j .text/.data -S obj 输出指定段的信息，大概就是反汇编源代码把objdump -S obj C语言与汇编语言同时显示 查看是否调用某个指令集：objdump -d combin | grep avx objdump -d 反汇编程序，从机器码生成汇编代码 nm显示可执行文件的符号表最好用gcc -g 编译之后在用nm 编译结果文件，查看文件的符号表，gcc -g会加入更多的符号表信息gdb就是通过读取符号表来查看程序的调用栈的信息 strings打印文件中可打印的字符strings 源代码 就是打印出源代码了strings 编译结果文件 就是把二进制文件中可以打印的部分文本给打印出来了]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS]]></title>
    <url>%2F2018%2F05%2F21%2FHDFS%2F</url>
    <content type="text"><![CDATA[介绍使用bigdl的时候需要将训练数据集放在分布式文件系统中 准备环境保证各个节点之间免ssh登陆单节点搭建hadoophttps://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html多节点搭建hadoop namenode系统环境变量：export JAVA_HOME=/root/jdk1.8.0_161export CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:.export HADOOP_PREFIX=/home/hadoop/hadoop-2.6.5export HADOOP_HOME=$HADOOP_PREFIX/binexport HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoopPATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH:$HOME/bin:/home/jiahaofa/spark/spark-2.2.1-bin-hadoop2.7/bin:$HADOOP_HOME:$HADOOP_PREFIX/sbinexport PATH 配置hadoopetc/hadoop/hadoop-env.sh 写入 export JAVA_HOME=/usr/java/latest 配置etc/hadoop/slaves文件r02s01r02s02 配置etc/hadoop/core-site.xml fs.defaultFS hdfs://r02s01:9000 io.file.buffer.size 131072 hadoop.tmp.dir file:///home/hadoop/hadoop-2.6.5/tmp 配置 etc/hadoop/hdfs-site.xml dfs.namenode.name.dir file:/home/hadoop/hadoop-2.6.5/tmp/dfs/name dfs.datanode.data.dir file:/home/hadoop/hadoop-2.6.5/tmp/dfs/data datanode系统环境变量：系统环境变量：export JAVA_HOME=/root/jdk1.8.0_161export CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:.export HADOOP_PREFIX=/home/hadoop/hadoop-2.6.5export HADOOP_HOME=$HADOOP_PREFIX/binexport HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoopPATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH:$HOME/bin:/home/jiahaofa/spark/spark-2.2.1-bin-hadoop2.7/bin:$HADOOP_HOME:$HADOOP_PREFIX:sbinexport PATH 配置hadoopetc/hadoop/hadoop-env.sh 写入 export JAVA_HOME=/usr/java/latest 配置etc/hadoop/core-site.xml 一定要配置，很重要 fs.defaultFS hdfs://r02s01:9000 不配置 etc/hadoop/hdfs-site.xml 启动：$HADOOP_PREFIX/bin/hdfs namenode -format$HADOOP_PREFIX/sbin/start-dfs.sh 启动之后用jps查看namenode和datanode的进程 关闭：$HADOOP_PREFIX/sbin/stop-dfs.sh 如果出现namenode或者datanode无法启动，查看机器上的log有明确信息 hdfs命令测试namenode:hdfs dfs -put ./testdata/lesliename.txt /其它节点hdfs dfs -ls /hdfs dfs -cat /lesliename.txt datanode:启动spark-shell：val textFile = spark.read.textFile(“hdfs://r02s01:9000/lesliename.txt”)textFile.first() 输出文件内容说明HDFS部署成功]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高并发系统开发]]></title>
    <url>%2F2018%2F04%2F12%2F%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[分布式系统https://www.zhihu.com/question/23645117 两个系统之前如何实现事务的同步eBay架构看一个例子：中行账户A向建行账户B转账1000元，应该如何实现，首先，中行本地有两张表，第一张表是账户表，第二张表是消息表。 开始转账后，中行账户A扣款1000元，通过本地事务将事务消息插入至消息表。 本地通常通过消息MQ(消息队列)的方式发送异步消息通知建行账户B，增加1000元。对方订阅并监听消息后自动触发转账的操作，为了保证幂等性，防止触发重复的转账操作，需要在执行转账操作方新增一个trans_recv_log表用来做幂等，在第二阶段收到消息后，通过判断trans_recv_log表中的字段来检测相关记录是否被执行，如果未被执行则会对B账户余额执行加1000元的操作，并会将该记录增加至trans_recv_log,事件结束后通过回调更新trans_message的状态值。 在上述例子中，转账方由于网络原因出现多次发送请求的情况怎么处理在中行账户中，每次请求都有个ID，hash值，在收款方的trans_recv_log表首先查询是否有这行请求的记录，没有的话就插入记录，并将这行的状态位标志为未处理，然后开始处理业务，在业务处理完成后，将状态位标志为已经处理。这样当重复请求过来，查看到状态位为已经处理，就不再处理业务。 在上述例子中，两次重复请求之间时间间隔很短怎么办，也就是第二次请求发过来的时候，第一次请求还在处理业务，还没来得及回调更新trans_recv_log表的标志位给trans_recv_log表的每一行加锁，这样第二请求来的时候就会等待，等到第一个请求完成，第二请求查询到标志位已经完成就不会触发业务了 在上述例子中，可能存在一个情况，就是第一个请求刚处理完业务，给行解锁准备更新标志位的时候，第二请求来了，因为还没来的及更新标志位，所以第二个请求会触发业务，业务被击穿了在业务请求之前，一定要写上锁，再查询在业务完成之后，一定要先更新状态，再解锁]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++内联汇编]]></title>
    <url>%2F2018%2F04%2F08%2FC-C-%E5%86%85%E8%81%94%E6%B1%87%E7%BC%96%2F</url>
    <content type="text"><![CDATA[工具gcc asm去编译汇编，这个叫做汇编器编译器将C代码编译成汇编代码，汇编器将汇编代码编译成机器码教程： https://www.ibm.com/developerworks/cn/aix/library/au-inline_assembly/index.htmlhttps://www.jianshu.com/p/1782e14a0766 代码AT&amp;T格式汇编123456789101112#include &quot;stdio.h&quot;void main()&#123; int a=10, b; printf(&quot;%d&quot;,a); asm(&quot;movl %1, %%eax \n\t&quot; &quot;movl %%eax, %0&quot; :&quot;=r&quot;(b) /* output */ :&quot;r&quot;(a) /* input */ :&quot;%eax&quot; /* clobbered register */ ); printf(&quot;%d&quot;,b);&#125; 编译运行12gcc test.c -o test./test intel格式汇编 https://blog.werner.wiki/72829768/1234567891011121314#include &quot;stdio.h&quot;void main()&#123; //printf(&quot;hello world!&quot;); int a=10, b=0; printf(&quot;%d \n&quot;,b); asm(&quot;mov eax,5 \n\t&quot; &quot;mov eax,%1 \n\t&quot; &quot;mov %0,eax&quot; :&quot;=r&quot;(b) :&quot;r&quot;(a) :&quot;eax&quot; ); printf(&quot;%d \n&quot;,b);&#125; 编译运行12gcc -masm=intel test.c -o test./test intel格式汇编与AT&amp;T格式汇编的区别https://blog.csdn.net/tigerjibo/article/details/7708811https://blog.csdn.net/tianshuai1111/article/details/7900084 C转汇编1,把.c程序转变为AT&amp;T格式汇编.s[root@xxx asm_study]# gcc -S asm.c -o asm.s[root@xxx asm_study]# ls -al asm.s-rw-r–r– 1 root root 1387 06-30 10:41 asm.s 2,把.c程序转变为Intel格式汇编.s[root@xxx asm_study]# gcc -masm=intel test.c -o test.s当然，要想把c程序转为Intel汇编时，其中不能包含AT&amp;T格式的汇编，否则无法转。 C++AT&amp;T风格123456789101112131415#include &lt;iostream&gt;using namespace std;void main()&#123; int a=10,b=0; cout&lt;&lt;b&lt;&lt;endl; asm(&quot;movl %1, %%eax \n\t&quot; &quot;movl %%eax, %0&quot; :&quot;=r&quot;(b) /* output */ :&quot;r&quot;(a) /* input */ :&quot;%eax&quot; /* clobbered register */ ); cout&lt;&lt;b&lt;&lt;endl; return;&#125; 编译12icpc test.cpp -o test3./test3 intel风格123456789101112131415#include &lt;iostream&gt;using namespace std;void main()&#123; int a=10,b=0; cout&lt;&lt;b&lt;&lt;endl; asm(&quot;mov eax, %1 \n\t&quot; &quot;mov %0, eax&quot; :&quot;=r&quot;(b) /* output */ :&quot;r&quot;(a) /* input */ :&quot;eax&quot; /* clobbered register */ ); cout&lt;&lt;b&lt;&lt;endl; return;&#125; 编译运行12icpc -masm=intel test.cpp -o test3./test3 读取cpuid原理：读cpu的信息只需要一条汇编指令 cupid ,入口参数在EAX寄存器，返回的信息在EAX，EBX，ECX，EDX寄存器，也就是说执行cupid之前先给EAX寄存器赋值，在执行cupid，执行过之后，cpu的相关信息就在EAX，EBX，ECX，EDX寄存器中了，入口参数EAXhttps://blog.csdn.net/fisher_jiang/article/details/4348194https://blog.csdn.net/razor87/article/details/87117121234567891011121314151617181920#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;void main()&#123; //string message; int a=10; int iEAXValue,iEBXValue,iECXValue,iEDXValue; asm volatile(&quot;mov eax,0 \n\t&quot; &quot;cpuid \n\t&quot; &quot;mov %0,ebx \n\t&quot; &quot;mov %1,ecx \n\t&quot; &quot;mov %2,edx&quot; :&quot;=r&quot;(iEBXValue),&quot;=r&quot;(iECXValue),&quot;=r&quot;(iEDXValue) :&quot;r&quot;(a) :&quot;eax&quot; ); cout&lt;&lt;hex&lt;&lt;iEBXValue&lt;&lt;endl&lt;&lt;iECXValue&lt;&lt;endl&lt;&lt;iEDXValue&lt;&lt;endl; return;&#125;]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[powerManagement]]></title>
    <url>%2F2018%2F03%2F22%2FpowerManagement%2F</url>
    <content type="text"><![CDATA[Power mode设置cpupower模式为performance或者powersavecpupower frequency-set -g performance(powersave) 修改kernel的启动参数如何添加kernel启动的参数vi /boot/efi/EFI/redhat/grub.cfg找到对应的启动的kernel找到这一行linuxefi /vmlinuz-3.10.0-514.6.1.el7.x86_64 root=UUID=b3343e70-94d7-4506-88e2-025037204d23 ro crashkernel=auto rhgb quiet LANG=en_US.UTF-8 在这一行末尾添加参数，，以intel_pstate为例（https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html）linuxefi /vmlinuz-3.10.0-514.6.1.el7.x86_64 root=UUID=b3343e70-94d7-4506-88e2-025037204d23 ro crashkernel=auto rhgb quiet LANG=en_US.UTF-8 intel_pstate=disable reboot机器之后会生效 Driver有两种power manager的driverintel_pstate 和 acpi-cpufreq 通过 cpupower frequency-info 可以查看到当前使用的驱动通过在kernel设置intel_pstate=disable来使用acpi-cpufreq intel_pstate会使用hardware P state control (HWP)去控制cpu的P stateacpi-cpufreq 不会使用hardware P state control (HWP) 使用intel_pstate时，fixfreq的脚本不起作用只有set 0x774寄存器set 一下fixfreq的脚本rdmsr -a 0x774rdmsr -a 0x198 读取0x198的寄存器看频率数据cpupower monitor(freqency-info) 去看看会看到数值之间有差距 使用acpi-cpufreq时，fixfreq的脚本起作用set 一下fixfreq的脚本rdmsr -a 0x774rdmsr -a 0x198 读取0x198的寄存器看频率数据cpupower monitor(freqency-info) 去看看]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Spark机群]]></title>
    <url>%2F2018%2F03%2F01%2F%E6%90%AD%E5%BB%BASpark%E6%9C%BA%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[介绍搭建一个spark机群进行数据分析结构：主节点+2个工作节点首先保证3个节点之间可以ssh免密码登录 依赖安装 主节点： java mvn：有必要配置代理,.m2/settings.xml spark：用做命令行测试，开发java程序，用mvn下载依赖包 工作节点： java spark： 启动salve并连接主节点正确安装各个依赖包之后，需要正确配置环境变量。 测试是否正确安装主节点 测试spark命令行，参考文档 测试spark用mvn构建，参考代码1234mvn packagespark-submit --class &quot;org.intel.dcg.leslie.SimpleApp&quot; --master local[4] target/simple-project-1.0.jar--class 指定jar包入口的class类--master local[4] 运行在本地的4线程 工作节点将主节点构建的jar包拷贝到工作节点123spark-submit --class &quot;org.intel.dcg.leslie.SimpleApp&quot; --master local[4] target/simple-project-1.0.jar# --class 指定jar包入口的class类# --master local[4] 运行在本地的4线程 构建机群参考文档 工作原理 运行命令行 standalone 主工具 步骤 首先配置cluster managers，参考standalone 主工具1.1 sbin/start-master.sh通过查看log可以看到master的url12345# start commandcd $spark_home/sbin./start-master.sh# read logvim $spark_home/logs/spark-root-org.apache.spark.deploy.master.*.log 从log里面得到启动的master的url： spark://headnode:7077这个变量要set到代码的SparkContext的master的变量里面1.2 编写 conf/slaves file1234vim $spark_home/conf/slaves#contentknm009knm010 1.3 在headnode上运行sbin/start-slaves.sh 运行所有slaves在worknode的节点上查看slaves的所有的log以及java进程12cd $spark_home/sbin./start-slaves.sh 用运行命令行启动应用12345678910# code:https://github.com/Leslie-Fang/basicSparkcluster branch# buildmvn package# how to run:# 跑在worknodes上面spark-submit --class &quot;org.intel.dcg.leslie.SimpleApp&quot; --master spark://headnode:7077 target/simple-project-1.0.jar# 跑在本地spark-submit --class &quot;org.intel.dcg.leslie.SimpleApp&quot; --master local[4] target/simple-project-1.0.jar 坑问题：worknode的log里看到无法连接Failed to connect to master headnode:7077http://blog.csdn.net/ybdesire/article/details/70666544需要关闭headnode的防火墙关闭之后将headnode上worknode上的spark进程都关闭关闭之后，重启headnode上的spark-master以及spark-client 搭建 BigDLBigDL是基于spark的进行DL的框架参考文档 安装 12345git clone https://github.com/intel-analytics/BigDL.gitcd $BigDL_Home# buildbash make-dist.sh -P spark_2.x#生成一个dist的folder 使用入门 setup spark的cluster 下载数据集合 运行spark，指定bigDL的jar包构建BigDL的jar包，和spark的multi-node一样的方式运行 1spark-submit --master spark://headnode:7077 --executor-cores 1 --total-executor-cores 2 --class com.intel.analytics.bigdl.models.vgg.Train dist/lib/bigdl-0.3.0-SNAPSHOT-jar-with-dependencies.jar -f /home/lf/bigdl/data/cifar-10-batches-bin -b 2 坑java虚拟机的堆空间溢出java.lang.OutOfMemoryError: Java heap space]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins]]></title>
    <url>%2F2018%2F02%2F27%2FJenkins%2F</url>
    <content type="text"><![CDATA[介绍Jenkins是一个自动化构建，测试工具 官网地址 教程https://jenkins.io/doc/pipeline/tour/hello-world/#examples 启动服务器： java -jar jenkins.war –httpPort=8080 启动服务器之后在页面上配置代理： skip配置代理先 用户名:leslie 123456 进入之后看到用户管理页面 新建一个项目 在项目页面上构建build的命令 build之后可以在项目页面中，进入build结果的页面 查看build结果中命令行的输出 配置maven代理： 公司内网没有配置成功，还是无法连接上maven的中心仓库 和github的集成可以在线安装插件有时候代理不可用的时候也可以离线安装plugin https://stackoverflow.com/questions/14950408/how-to-install-a-plugin-in-jenkins-manually 插件下载地址： https://updates.jenkins-ci.org/download/plugins/git/安装完插件之后如何和集成： https://www.jianshu.com/p/2836551a45ba]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[odata]]></title>
    <url>%2F2018%2F02%2F05%2Fodata%2F</url>
    <content type="text"><![CDATA[介绍开放数据协议（Open Data Protocol，缩写 OData）是一种描述如何创建和访问 Restful 服务的 OASIS 标准。sapui5和后端通过odata作为接口，odata的wiki介绍,官网教程 sapui5+odataopenui5的文档搜索odata可以看到如何使用odata和后端交互数据推荐使用：sap.ui.model.odata.v2.ODataModel]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark]]></title>
    <url>%2F2018%2F01%2F30%2FSpark%2F</url>
    <content type="text"><![CDATA[Install Download &amp; uncompressed jdk set the environment variables of java 123456export JAVA_HOME=/home/automation/java/jdk1.8.0_161export CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:.export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHexport SBT_HOME=/home/automation/spark/sbt/sbtexport SPARK_HOME=/home/automation/spark/spark-2.2.1-bin-hadoop2.7export PATH=$PATH:$SBT_HOME/bin:$SPARK_HOME/bin Download &amp; uncompressed spark set the environment variables of spark install sbt： used to build scala Doc官方入门文档 打开命令行交互dataset的构造函数中传入输入文件，调用dataset的[API处理文档]（https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset） 编写可运行的文件 build.sbtSimpleApp.scala放在规定的目录结构下面12345678910111213import org.apache.spark.sql.SparkSessionobject SimpleApp &#123; def main(args: Array[String]) &#123; val logFile = &quot;YOUR_SPARK_HOME/README.md&quot; // Should be some file on your system val spark = SparkSession.builder.appName(&quot;Simple Application&quot;).getOrCreate() val logData = spark.read.textFile(logFile).cache() val numAs = logData.filter(line =&gt; line.contains(&quot;a&quot;)).count() val numBs = logData.filter(line =&gt; line.contains(&quot;b&quot;)).count() println(s&quot;Lines with a: $numAs, Lines with b: $numBs&quot;) spark.stop() &#125;&#125; sbt package 的时候如果有些包因为proxy无法下载将包离线下载下来以后放到对应的目录下面 ~/.sbt/preloaded/*]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MKL]]></title>
    <url>%2F2018%2F01%2F19%2FMKL%2F</url>
    <content type="text"><![CDATA[介绍MKL是intel开发的一个数学运算库，集成在psxe软件包中。提供了一写API供开发者调用。 安装首先安装psxe包含了所有的原件，或者单独安装mkl以及icc 如何使用MKL入门教程 C语言：在代码投include mkl的头文件： #include “mkl.h”在代码中调用mkl的API 编译：source MKL的环境，主要配置环境变量，设置动态链接库的路径：source /opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/bin/mklvars.sh intel64 source一下iccsource /opt/intel/bin/iccvars.sh intel64 直接编译文件：icc mkl-lab-solution.c -mkl用makefile肯定也是类似的 生成一个可执行文件：a.out可以直接运行：./a.out 1000]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MKLDNN]]></title>
    <url>%2F2018%2F01%2F19%2FMKLDNN%2F</url>
    <content type="text"><![CDATA[介绍MKLDNN是一个Intel开发的基于Intel芯片架构的开源库，提供用于深度学习的API.Intel 开源软件中心MKLDNN Github 主页 编译安装mkldnn123451. Git clone mkldnn2. source psxe3. Cd mkldnn &amp;&amp; mkdir build &amp;&amp; cd build4. CC=icc CXX=icpc cmake .. -DCMAKE_INSTALL_PREFIX=../mkldnn_install/5. CC=icc CXX=icpc make -j 66 &amp;&amp; make install make install之后：如果第二步没有指定目录，会安装在/usr/local目录下面Shared libraries (/usr/local/lib):头文件：Header files (/usr/local/include):文档:Documentation (/usr/local/share/doc/mkldnn): 编译链接代码：入门文档这个文档内容有点问题：具体编译的命令看github的readme：https://github.com/01org/mkl-dnn首先配置环境变量12345678source psxe(应该只需要mkl模块)export MKLDNNROOT=/home/automation/mkldnn/selfBuilt/mkl-dnn-0.12/mkldnn_installG++ 编译g++ -std=c++11 -I$&#123;MKLDNNROOT&#125;/include -L$&#123;MKLDNNROOT&#125;/lib simple_net.cpp -lmkldnn -o ./bin/simple_net_cpp用icc编译的话：icpc -std=c++11 -I$&#123;MKLDNNROOT&#125;/include -L$&#123;MKLDNNROOT&#125;/lib simple_net.cpp -lmkldnn -o ./bin_icc/simple_net_cpp 编译完的运行：报错找不到mkldnn的动态链接库添加动态链接库12345678export LD_LIBRARY_PATH=$LD_LIBRARY_PATH://home/automation/mkldnn/selfBuilt/mkl-dnn-0.12/mkldnn_install/lib报错找不到mkl_rt：source 一下mkl或者psxe然后运行：./bin_icc/simple_net_cpp运行成功，不会报错 Simple_Net Code API 代码的解释https://software.intel.com/en-us/articles/intel-mkl-dnn-part-2-sample-code-build-and-walkthrough MKLDNN框架介绍 Caffe调用MKLDNN可以我的另外一篇博客参考：Caffe中的生产者模式 caffe的layer_factory会去创建mkldnn的层 caffe在调用layer.forward函数的时候会调用到对应mkldnn的层的forward_cpu函数 在这个函数中(initxxxpd)的方法会先去判断对应的pd(privimite descriptive)和privimite是否存在，如果不存在的话会去创建对应的pd 在initxxxpd方法中会去调用reset的方法去创建pd(privimite descriptive)和privimite reset方法的传入的参数就是new出来的一个新的mkldnn的primitive进入mkldnn 在new一个新的mkldnn的primitive的时候，可以看到create后缀或者init后缀的函数 在这些create函数中最后会去调用一个privimite-&gt;create_primitivate的函数，这是个虚函数 这个虚函数和具体的运算kernel之间如何调用实现的，暂时不是很清楚，应该有复杂的调用关系，但是核心的思想是：在调用privimite-&gt;create_primitivate的时候会去遍历所有的kernel，在每个kernel的函数中都有一个init_conf的函数，在遍历所有的kernel的时候，会去看这个init_conf的函数是否满足条件，满足条件就意味着调用这个kernel，否则的话就看下个kernel]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链]]></title>
    <url>%2F2018%2F01%2F17%2F%E5%8C%BA%E5%9D%97%E9%93%BE%2F</url>
    <content type="text"><![CDATA[入门资料 什么是区块链https://www.youtube.com/watch?v=chQttZ4PH24]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kernel学习笔记]]></title>
    <url>%2F2018%2F01%2F07%2Fkernel%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[用户态与内核态http://jakielong.iteye.com/blog/771663 linux启动过程https://www.youtube.com/watch?v=yeMA7AJFtb8https://www.youtube.com/watch?v=ALzySqflHJwyoutube:Basics of the Linux Boot Process 内存管理http://gityuan.com/2015/10/30/kernel-memory/youtube的教程：Virtual Memory，包括TLB的使用 Hyper-threading 技术Intel的CPU可以再BIOS仲打开hyper-threading比如说2socket，每个socket有28个物理的core，打开hyper-threading之后可以有56个逻辑core，一共是112个逻辑core可以通过 1numactl --hardware 查看socket 0上的逻辑core的编号是0-27,56-83socket 1上的逻辑core的编号是28-55,84-111可以通过1taskset -c 0,56(逻辑core的编号) process 将特定的process运行在这些逻辑core上面 进程与线程进程通过调用fork这个kernel的api实现线程通过调用Pthreads这个kernel的api实现通过top可以看到进程的pid通过top -p 进程pid可以看到这个进程包含的线程]]></content>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mango's Journey of Python]]></title>
    <url>%2F2017%2F12%2F10%2FforMango%2F</url>
    <content type="text"><![CDATA[IntroductionThis is a python learning plan for Mango who has a dream of being an professional analyst. Rome Was not Building in a Day. Wish you to be dedicated and enjoy the gain after pay. Main Content Python basic concept(5 weeks) Flask Module and Web develop(1 week) Numpy Module(2 weeks) Pandas Module(1 week) AI Introduction(1 week) Weekly PlanHere we would use World Standards Week ,which is in short of WW, to count learning progress. World Week Plan WW50 PyCharm installation: pycharm is my favourite develop tool for python Python Introduction Variable and expression WW51 Condition(If..else..) Loop(for and while) 2017WW52-2018WW1 Merry Christmas and Enjoy the holiday WW2 Python FunctionData Structure WW3 Python modules WW4 Flask Module and Web develop WW5 Python Class WW6 Review and independent Task（Greedy Snake） WW7-8 Spring Festival WW9-10 Review and independent Task WW11 Introduction of Numpy WW12 Introduction of Pandas WW13 Introduction of AI with the application of numpy and pandas]]></content>
      <tags>
        <tag>Daily</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ncurses]]></title>
    <url>%2F2017%2F12%2F04%2Fncurses%2F</url>
    <content type="text"><![CDATA[介绍ncurses是一个库提供了API进行终端(termal)的编程，可以看到类似于我们在终端编译内核的时候出现的选项界面。除了C语言，还可以用nodejs，python等去调用. 参考文章https://www.gnu.org/software/ncurses/http://tldp.org/HOWTO/NCURSES-Programming-HOWTO/]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numactl]]></title>
    <url>%2F2017%2F12%2F02%2Fnumactl%2F</url>
    <content type="text"><![CDATA[介绍NUMA(Non-Uniform Memory Access)字面直译为“非一致性内存访问”对于多核CPU而言，它的作用可以将程序绑定在固定的CPU的进程以及固定的内存块上，避免了程序在不同CPU和内存块之间切换导致的时间消耗。从而可以提高程序的性能 查看cpu的配置1numactl --hardware 来看两类CPU的值： CPU1，物理上有两个socket，每个socket 28core，每个core可以超线程2注意socket0，1上超线程的那个线程的值是错开的0,56在同一个物理core上面123456789101112131415161718192021available: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83node 0 size: 96071 MBnode 0 free: 93860 MBnode 1 cpus: 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111node 1 size: 0 MBnode 1 free: 0 MBnode distances:node 0 1 0: 10 21 1: 21 10 这里我们希望特定的程序运行在特定的memory以及特定的线程上面可以使用这样的命令1numactl -m 0(1) taskset -c 0,56 command to start the program CPU2，物理上只有1个socket，每个socket 72core，每个core可以超线程4所以所有的CPU的进程都在CPU上面但是CPU2在CPU内部还有内存，所以看到CPU2上进程数为0，但是有memory 123456789101112131415161718192021available: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287node 0 size: 193306 MBnode 0 free: 191208 MBnode 1 cpus:node 1 size: 16125 MBnode 1 free: 15831 MBnode distances:node 0 1 0: 10 31 1: 31 10 所以如果希望程序使用CPU内部的这部分内存可以这样1numactl -m 1 command to start the program auto numa balanceredhat系统默认打开auto numa balance理论上使用numactl的时候就不会用到auto numa balance numastatnumastat查看numa的使用情况博客资料http://dupengair.github.io/2016/10/12/%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E5%B7%A5%E5%85%B7%E7%AF%87-numastat/]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe]]></title>
    <url>%2F2017%2F11%2F11%2Fcaffe%2F</url>
    <content type="text"><![CDATA[介绍Caffe 是Berkeley Vision and Learning Center(BVLC)开发的一个深度学习的框架，github地址,最初是用于图像处理，也可以用于自然语言处理。英特尔根据自己的CPU对caffe做了优化，并发布了intelcaffe版本，调用MKL以及MKLDNN库，从而编译后使用最新的指令集加速运算。 基本概念 Blob就是caffe封装的数据，默认的输入图像的格式是(NCHW):number N x channel K x height H x width W solver: 定义一个solver文件,里面指定使用哪个net，步长多少 安装 安装一些系统依赖包 下载Intel caffe的源码 cp Makefile.config.example Makefile.config 修改Makefile.config中的编译选项 make -j 270 all（指定编译过程中可以使用的进程数量） 手写数字识别(Mnist)准备数据下载数据运行这个脚本$CAFFE_ROOT/data/mnist/get_mnist.sh下载数据集到当前目录下面$CAFFE_ROOT/data/mnist/ 转换数据格式运行这个脚本去转换数据格式$CAFFE_ROOT/examples/mnist/create_mnist.sh会在$CAFFE_ROOT/examples/mnist/目录下面生成两个.lmdb后缀的文件，里面就是训练集和测试集 LeNet模型要使用caffe，最重要的就是定义模型，也就是编写对应的.prototxt文件$CAFFE_ROOT/examples/mnist/lenet_train_test.prototxt LeNet的求解器solversolver定义了求解的参数$CAFFE_ROOT/examples/mnist/lenet_solver.prototxt 运行的环境变量设置与脚本./examples/mnist/train_lenet.sh在solver中有指定训练多少itertaion之后保存一次模型参数可以用于后面进一步的训练或者inference]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译Ubuntu内核]]></title>
    <url>%2F2017%2F10%2F30%2F%E7%BC%96%E8%AF%91Ubuntu%E5%86%85%E6%A0%B8%2F</url>
    <content type="text"><![CDATA[介绍根据使用需要，有时候关闭一些内核选项，然而在OS没有提供对应工具的情况下，就需要重新编译linux的kernal 查看内核版本1234# 查看内核版本uname -r# 查看OS版本lsb_release -a 下载源码apt-get install linux-source下载内核默认下载到 /usr/src 目录 解压缩包12tar xf linux-.*.tar.xz -C /usr/src/ln -sv linux-.* linux # 有需要的话需要创建一个链接 配置内核选项并编译配置内核的方法的编译选项的方法有好多种，下面的每一种make就对应了一种方法，只需要从里面选一种就可以了，最常见的就是make menuconfig, 但是需要安装cc和ncurses-devel这两个包12345678make config：遍历选择所要编译的内核特性make allyesconfig：配置所有可编译的内核特性make allnoconfig：并不是所有的都不编译，而是能选的都回答为NO、只有必须的都选择为yes。make menuconfig：这种就是打开一个文件窗口选择菜单，这个命令需要打开的窗口大于80字符的宽度，打开后就可以在里面选择要编译的项了下面两个是可以用鼠标点选择的、比较方便哦：make kconfig(KDE桌面环境下，并且安装了qt开发环境)make gconfig(Gnome桌面环境，并且安装gtk开发环境)menuconfig：使用这个命令的话、如果是新安装的系统就要安装gcc和ncurses-devel这两个包才可以打开、然后再里面选择就可以了、通这个方法也是用得比较多的： 另外一种常见的方法是基于当前OS的编译配置选项去修改，复制当前系统上的/boot/config-版本-平台编译选项文件，将这个文件复制到/usr/src/linux/.config覆盖./config这个文件 用screen开一个子窗口，在子窗口中编译1make -j144 all &amp;&gt; make_record.txt 这一步不确定是否需要,目前感觉不一定需要make 之后 在 make install之前还需要make bzImage一下，不然会报错* Missing file: arch/x86/boot/bzImage 编译后的安装12345make modules_install #这一步是需要的 这步完了之后你可以查看一下/lib/modules/目录下就会生成一个以版本号命名的一个文件模块了make install 安装完之后会在/boot/目录下生成一个内核文件vmlinuz-3.13.2、还有几个跟你当前编译的版本一样的文件、可以ls去看一下：ls /boot/ 编译好了一个新内核了之后可以到grub.conf配置文件时看一下： vim /boot/grub/grub.conf参考文档2 重启系统，在重启界面上可以看到各个可以启动的内核的信息，选择我们刚刚编译好的这个内核如果在第一层没有看到内核的选项，进入到advance的里面一级选项，里面会看到编译好的内核的启动选项 删除不需要的kernal直接在/boot下面把对应的文件删除，一个kernal应该有三个文件然后在/boot/grub/grub.cfg文件里面把对应的启动项给删除了修改grub.cfg是只读属性chmod 644 grub.cfg改完保存之后chmod 444 grub.cfg 重新改成只读的属性启动之后看到旧的选项被删除了]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[受限玻尔兹曼机]]></title>
    <url>%2F2017%2F10%2F21%2F%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[介绍受限玻尔兹曼机常用作无监督学习.包含两层，可见层和隐藏层，层内神经元无连接，层间神经元全连接，因此受限玻尔兹曼机对应了一个二分图. 核心概念调整参数，使得用RBM表示的概率分布尽可能的和训练数据集相符合。核心训练算法采用对比散度法 代码参考参考github仓库-fork from xidui.]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同一OS下不同python2.7与3.4]]></title>
    <url>%2F2017%2F10%2F15%2F%E5%90%8C%E4%B8%80OS%E4%B8%8B%E4%B8%8D%E5%90%8Cpython2-7%E4%B8%8E3-4%2F</url>
    <content type="text"><![CDATA[介绍目前大部分程序都是基于python2.7开发的，但是python2只维护到2020年，未来是python3的，本文只要为了在一台机器上同时安装python2和python3，避免发生包冲突的问题。 安装可以参考链接1macos上默认安装的就是python2.7直接1brew install python3 安装完成之后python -V看到的还是python2.7.10 安装完成之后python3 -V看到python3的版本 强烈建议使用virtualenv链接 创建python2的虚拟环境 123pip install virtualenvpython -Vvirtualenv --no-site-packages venv3 --python=python2.7 创建python3的虚拟环境 123pip3 install virtualenvpython3 -Vvirtualenv --no-site-packages venv3 --python=python3.6]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_virtualenv]]></title>
    <url>%2F2017%2F10%2F15%2Fpython-virtualenv%2F</url>
    <content type="text"><![CDATA[介绍为了解决不同python程序之间，所依赖的包的版本不同导致程序无法运行的问题。 解决方法解决方法可以是将在现有的程序代码的目录下运行 创建创建一个venv目录，这个目录表示的就是一个python虚拟环境1virtualenv --no-site-packages venv 启动虚拟环境一下命令可以启动虚拟环境1source venv/bin/activate 关闭虚拟环境关闭虚拟环境1deactivate 删除直接删除venv文件夹及可以 开发在pycharm的default设置中可以选择到这个虚拟环境中的python，来解析pcharm中的包的依赖问题]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgresql学习笔记]]></title>
    <url>%2F2017%2F10%2F15%2Fpostgresql%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[介绍PostgreSQL相对于竞争者的主要优势为可编程性：对于使用数据库资料的实际应用，PostgreSQL让开发与使用变得更简单。微信支付核心数据库也是基于 PostgreSQL链接 安装ubuntu下面: 安装客户端sudo apt-get install postgresql-client 安装服务器sudo apt-get install postgresql安装完成后，PostgreSQL服务器会自动在5432端口开启，所以如果在阿里云上部署，需要远程访问的话记得在阿里云管理界面上管理规则组中打开5432端口 创建新用户和数据库安装postgresql之后默认生成一个名为postgres的数据库和一个名为postgres的数据库用户，同时还生成了一个名为postgres的Linux系统用户，有两种方法使用shell命令行的方法 创建数据库用户创建数据库用户leslie，并指定其为超级用户1sudo -u postgres createuser --superuser leslie 如果原来是在root用户下12su - postgrescreateuser --superuser leslie_postgres3 登录数据库控制台,设置账号密码psql命令登录PostgreSQL控制台123sudo -u postgres psql #login the PostgreSQL Console\password leslie_postgres3 # enter new password 123456\q #leave the PostgreSQL Console 创建数据库123sudo -u postgres createdb -O leslie_postgres3 testdb2orsu - postgres createdb -O leslie_postgres3 testdb2 登录数据库12345psql -U dbuser -d exampledb -h 127.0.0.1 -p 5432# -U username# -d database name# -h ip# -p port 常见console命令123456789101112\h：查看SQL命令的解释，比如\h select。\?：查看psql命令列表。 # 按q返回\l：列出所有数据库。\c [database_name]：连接其他数据库。\d：列出当前数据库的所有表格。\d [table_name]：列出某一张表格的结构。\du：列出所有用户。\e：打开文本编辑器。\conninfo：列出当前数据库和连接的信息。\q: 离开console\i: 从指定的文件中读取命令 比如:\i basics.sql直接使用SQL命令操作数据库 新特性 窗口函数1SELECT depname, empno, salary, **avg(salary) OVER (PARTITION BY depname)** FROM empsalary; 功能和group by 有点类似 继承123CREATE TABLE capitals ( state char(2)) INHERITS (cities); 这样capitals表继承了cities表在cities表查询数据的时候会获得所有的数据，如果用 only cities则只会包含cities表中的数据 开发接口标准的ODBC和JDBCpython 开发接口：psycopg模块官方教程入门教程]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MPI在高性能计算中的应用]]></title>
    <url>%2F2017%2F10%2F07%2FMPI%E5%9C%A8%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[介绍MPI(Message Passing Interface)在高性能计算中(High performance calculation 下面简称为HPC)具有广泛的应用。基本的应用范式可以分为两类: 单机多进程并行计算 多机集群并行计算 MPI的实现由以下几种库： MPICH open-mpi intel-mpi 其中前两个库是开源的，intel的库不是开源的，集成在intel的MKL当中 MPI安装可以通过两种方式安装： OS的软件库安装： ubuntu 的apt-get install MAC-OS的brew install redhat的yum install 通过源码包比如下载mpich的源码包,安装过程就是一般的老三套: configure 添加编译选项生成makefile文件 make编译 make install 安装 如果有必要，将mpi/bin目录添加到环境变量 MPI 编程基本的流程可以分成3个步骤 调用MPI的API，实现代码逻辑 编译:mpic++ -std=c++11 -o 可执行文件名字 源码 运行mpiexec -n 进程数 ./可执行文件 输入参数（mpiexec基本与mpirun等价） MPI的一般代码逻辑开启MPI:123MPI_Init(NULL, NULL);MPI_Comm_size(MPI_COMM_WORLD, &amp;comm_size);//comm_size is the number of processMPI_Comm_rank(MPI_COMM_WORLD, &amp;comm_rank);//comm_rank is the process&apos;s number 其中comm_size就是调用命令时传入的进程数量comm_rank是每个进程的标识分别从(0到comm_size-1) 逻辑块 一般的编程思路是将大量独立的计算分割成几个块，每个块分别放入到一个进程中进行并行计算，计算结束之后，在第一个进程(rank 0)中搜集各个进程的计算结果，归总判断，进行下一轮计算或者结束计算 在编程过程中可以通过comm_rank来判断当前处在哪个进程中 通过调用MPI_Barrier(comm)，其中comm为一个对象承担各个进程之间通信的任务，则会等待所有进程都执行到这一句代码，每个进程才会继续往下执行 数据的搜集:在每一轮计算之后，搜集各个进程的计算结果到rank 0中 123456789if(my_rank != 0)&#123; MPI_Send(&amp;has_change, 1, MPI_BYTE, 0, 0, comm);&#125;else&#123; bool localhas_change = false; for(int j=1;j&lt;p;j++)&#123; MPI_Recv(&amp;localhas_change, 1, MPI_BYTE, j, 0, comm, &amp;state); has_change = localhas_change | has_change; &#125;&#125; 每个进程都把各自的数据发出来，进程0搜集到每个进程发出来的数据之后，对数据进行逻辑运算 数据的广播:主要用于数据的同步，可以用在计算开始之前以及每一轮计算结束并搜集数据准备开始下一轮计算之前 1234567if(my_rank == 0)&#123; for(int k = 1; k &lt; p; k++)&#123; MPI_Send(&amp;has_change, 1, MPI_BYTE, k, 0, comm); &#125;&#125;else&#123; MPI_Recv(&amp;has_change, 1, MPI_BYTE, 0, 0, comm, &amp;state);&#125; 进程0将归纳好的数据发出来，其它进程搜集到这些数据之后进行数据的同步 结束MPI当计算结束之后，结束MPI1MPI_Finalize(); 实例代码可以参考我的github仓库，这个仓库中的serial_bellman_ford.cpp以及mpi_bellman_ford.cpp分别通过串行以及mpi并行的方式实现了Bellman-Ford算法计算单源图的最短路径。]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB调试程序]]></title>
    <url>%2F2017%2F09%2F25%2FGDB%2F</url>
    <content type="text"><![CDATA[介绍GDB可以用在linux shell下debug程序GDB简明教程：https://zhuanlan.zhihu.com/p/21987947https://www.jianshu.com/p/08de5cef2de9 编译程序在编译的时候手动加上-g选项1g++ -g file.cpp -o file 启动GDB环境进入GDB调试环境1gdb file #file为编译后的可执行文件 查看源码123list #查看后10行list - #查看前10行help list #查看帮助，看到可以列出指定文件指定的行，或者函数 添加断点123456789101112help break #查看帮助文档b function_nameb row_numb file_name:row_numb row_num if conditioninfo break #查看断点i b #查看断点，会返回断点的Numdisable 1 #禁用断点，Num 为info查看到的断点号d 1 #删除断点，Num 为info查看到的断点号 运行程序1234567run(r)#运行程序n #单步执行until NUM #离开循环，执行到指定的行step #跳入函数finish #离开函数quit #离开gdb环境c #跑到下一个断点 打印变量12345678910print var #打印变量默认打印var长度为200个字符show print elements查看设置无限制长度：set print elements 0watch var #监控变量info watch #查看监控的变量whatis var #查看变量类型x 虚拟地址 #查看特定虚拟地址上的值，默认一个值是4Bytesx/1sb addrhttps://blog.csdn.net/allenlinrui/article/details/5964046 打开图形化界面1wi #通过图形化界面查看debug过程更形象 查看调用栈1bt 具体列子调试C++程序，想看C++在什么时候读取的图片 直接在函数的位置打log gdb加断点写一个简单C函数打断点extern “C” my_debug() {} //用C函数，编译后符号表里面函数名方便认识，方便用b 函数名打断点mydebug() {print(__line)}my_debug() ;b my_debugbt gdb ./build/tools/caffeset args time -model ./caffe_validation_models/TrueImageModels/lmdb/default_resnet_50/deploy.prototxt –forward_only –phase TEST –iterations 100 –engine=MKLDNN 编译的时候去掉选项 -O2 -O3添加编译选项-g 查看文件中是否有O3find . -name “Makefile” | xargs grep O3查找所有makefile文件xargs命令的作用就是把每个查找到的文件送入grep 打印变量出错gdb 因为编译时的优化，在makefile里面把O2去掉或者改成O0或者makefile里面定义一个OPTIMIZATION变量OPTIMIZATION?=-O0make OPTIMIZATION=-O0]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置https的ssl证书]]></title>
    <url>%2F2017%2F09%2F25%2F%E9%85%8D%E7%BD%AEssl%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[使用nginx配置ssl证书https://aotu.io/notes/2016/08/16/nginx-https/index.html阿里云服务器配置https://www.cnblogs.com/tianhei/p/7726505.html]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx]]></title>
    <url>%2F2017%2F09%2F25%2FNginx%2F</url>
    <content type="text"><![CDATA[介绍一些web框架入django,flask，express等内部已经集成了一些web服务器，如：web服务器与应用程序交互规范：最早出现的是 CGI，后来又出现了改进 CGI 性能的FasgCGI，Java 专用的 Servlet 规范，Python 专用的 WSGI 规范等等但是这些web服务器社和用在开发环境不适合用在生产环境。在生产环境的用法是nginx+uWSGI+web应用框架程序 其中web应用程序的框架只提供服务 uWSGI作为web程序和nginx服务器之间的桥梁 ngnix作为服务器 安装配置 下载安装nginx官网https://www.nginx.com/resources/wiki/start/topics/tutorials/install/添加repo仓库，然后安装，根据平台不同，repo仓库的配置文件有一些对应的字段需要修改http://nginx.org/en/linux_packages.html基本的安装以及入门教程http://www.itzgeek.com/how-tos/linux/centos-how-tos/install-nginx-on-centos-7-rhel-7.html ubuntu:http://wiki.ubuntu.org.cn/Nginx通过apt-get install 安装也可以通过systemctl start(stop,reload,restart) nginx来控制ngnix配置文件目录：/etc/nginxlog文件目录：/var/log/nginx自带的html目录: /usr/share/nginx/html uWSGIuWSGI的介绍：https://github.com/unbit/uwsgi 安装pip install uwsgi报错: no python.h file，https://github.com/MeetMe/newrelic-plugin-agent/issues/151先装python-devel和装 python-devel,https://stackoverflow.com/questions/23215535/how-to-install-python27-devel-on-centos-6-5先找到适合的包，yum search python | grep -i devel然后Yum install 配置快速入门教程: http://uwsgi-docs.readthedocs.io/en/latest/WSGIquickstart.html这个快速入门教程里面介绍了如何使用:1.uWSGI+web程序框架 2.ngnix+uWSGI+web程序框架（flask,djano等） ngnix最好的nginx的教程：http://openresty.org/download/agentzh-nginx-tutorials-zhcn.html 三者集成的一些文章生产环境不使用flask自带的WSGI服务器，独立配置http://knarfeh.com/2016/06/11/%E5%86%99%E7%BB%99%E6%96%B0%E6%89%8B%E7%9C%8B%E7%9A%84Flask+uwsgi+Nginx+Ubuntu%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/flask官方：http://flask.pocoo.org/docs/0.12/deploying/uwsgi/入门配置nginx: https://segmentfault.com/a/1190000002411626 实例代码https://github.com/Leslie-Fang/D3Demo 仅仅使用uwsgi:uwsgi –http :5000 –manage-script-name –mount /=main:appuwsgi各个参数的含义参考的配置–http 指定端口–mount 绑定上特定的APP，其中main.py是文件名字，app是文件名中的APP的名字(app = Flask(name))–manage-script-name 经常和–mount一起使用 使用uwsgi+ngnixuwsgi:uwsgi –socket 127.0.0.1:5000 –wsgi-file main.py –callable app –processes 4 –threads 2 –stats 127.0.0.1:9191 –socket: 指定绑定的socket，在ngnix的配置中会使用到–wsgi-file： 指定web应用的入口文件–callable： 指定入口文件中的APP名字(app = Flask(name))–stats: 将uWSGI状态作为一个JSON对象导出到一个socket(http://uwsgi-docs-zh.readthedocs.io/zh_CN/latest/StatsServer.html),通过这个socket可以读取uwsgi的信息 ngnix:ubuntu的话/etc/ngnix/ngnix.conf文件在http块中添加server块： 12345678server&#123; listen 8070; server_name 0.0.0.0; location / &#123; include uwsgi_params; uwsgi_pass 127.0.0.1:5000; &#125;&#125; 解决实现负载均衡过程中的session问题 nginx使用iphash策略去分配访问流量iphash策略会将同一个ip的访问分配给同一台服务器，那么同一个用户每一次访问的session就是相同的缺点：要是再同一个用户访问期间，服务器挂了，这时候nginx会将访问分配给不同的服务器，session不一样了，用户需要重新登录 将session存在cookie里面将session存在cookie里面，但是要对cookie数据加密，不然session数据不安全优势：1.不存在session在服务器存储和同步问题 2.比如说这时候有两个web服务器，一个django写的，一个express写的，用户只要在一台服务器上登录成功，带着cookie访问另外一台服务器就自己会登录了缺点：每一次访问带的cookie数据量变大了，但是其实时间消耗也没多多少 将session存在redis里面，同时不同服务器之间的redis数据热同步redis之间共享数据也就2ms缺点：访问峰值的时候可能会很慢 第1，3个方法可以结合使用，第二个方法比较独立(leetcode就是这么实现负载均衡的) nginx 配置基础配置文档123456789101112131415server&#123; server&#123; listen 3389; server_name 0.0.0.0; error_page 404 /40x.html; location / &#123; # root /usr/share/nginx/html; index testnginx.html; &#125; location /index &#123; root /usr/share/nginx/html; index testnginx2.html; &#125; &#125;&#125; root如果不配置，就会默认使用nginx安装过程中的html目录，也就是自带的html目录: /usr/share/nginx/html,这个例子中将root配置成上述的绝对路径也是可以的 index配置：表示location后面没有带具体的资源的名字的时候(比如这个例子中第一个location的/，就符合条件，没有带上具体访问哪个资源)，则返回这个index配置的资源 server_name:如果nginx中只配置一个server域的话，则nginx是不会去进行server_name的匹配的。 第二个location，如果请求http://47.91.245.251:3389/index/，则会到/usr/share/nginx/html/index目录下面去查找，因为这个location也没有带上具体的资源，所以返回index对应的资源（这个请求和http://47.91.245.251:3389/index/testnginx2.html请求效果是一样的）]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2017%2F09%2F24%2Fnumpy%2F</url>
    <content type="text"><![CDATA[使用ipython进行数据分析ipython有shell的接口也有web的接口官网：https://ipython.org/ipython功能:https://www.zhihu.com/question/51467397 运行：jupyter notebook –ip=0.0.0.0 –allow-root 特殊用法 Tab键自动补全 内省，变量后跟？，显示变量的信息 入门教程：https://zhuanlan.zhihu.com/p/24988491书籍：https://detail.tmall.com/item.htm?_u=om6p4lp35f6&amp;id=36838488413 生成随机数numpy各种生成随机数的函数:http://www.jianshu.com/p/214798dd8f93numpy.random.seedhttps://www.reddit.com/r/learnpython/comments/3nidns/how_to_use_numpyrandomseed/用法：numpy.random.seed(num1)生成随机数下一次再生成随机数的时候再调用一次numpy.random.seed(num1)依然生成同样的随机数如果numpy.random.seed(num2)再生成随机数就是不同的随机数 Numpy.genfromtxt从文件读取数据http://www.jianshu.com/p/82110f1dbb94 array.tolist作用:convert a NumPy array to a Python Listhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tolist.htmlb = a.tolist()重新创建a = np.array(b) 广播法则处理不同维度的矩阵之间的运算https://ptorch.com/news/38.html 从文件读取数据np.genfromtxt(filename)http://www.jianshu.com/p/82110f1dbb94]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2017%2F09%2F21%2Fdocker%2F</url>
    <content type="text"><![CDATA[基本概念 docker daemon image container network label:https://docs.docker.com/engine/userguide/labels-custom-metadata/ storage driver:http://dockone.io/article/1765 volumes 常用命令 docker ps: 查看正在运行的container 通过dockerfile去定义image： https://docs.docker.com/get-started/part2/#apppy dockerbuild 利用dockerfile去生成image docker build -t friendlyhello . (friendlyhello 镜像名)important: 在dockerfile里面用pip去安装包的时候也需要配置代理 docker image ls 查看有没有生成镜像 docker run -p 4000:80 friendlyhello (-p 4000:80 把container的80端口映射到 主机的4000端口)docker run -d -p 4000:80 friendlyhello（-d 参数表示运行在后台，in detached mode） attach a containerdocker attach container_name这样就会进入contaienr内部要deattach这个container，ctrl-q docker container ls(只显示activate的container)查看所有的container: docker container ls -a查看到container id 停止正在运行的containerdocker stop container_id 删除containerdocker rm container_id 删除imagedocker rmi image_id（必须要保证这个image没有container在使用） 上传镜像docker login位image添加标签 docker tag image username/repository:tagdocker tag test1 lesliefang/get-started:v.0 docker image ls会看到这个lesliefang/get-started:v.0 image 上传docker push username/repository:tag登录docker hub 可以看到相关的信息 ## 创建services:scale our application by running this container in a service用service可以实现负载均衡一个service其实就是定义了一个镜像如何跑（运行在哪个端口，跑多少个container）用docker-compose.yml文件去定义: 创建完compose文件之后 docker swarm init（多张网卡的话：docker swarm init –advertise-addr 10.239.182.67） docker stack deploy -c docker-compose.yml appname(自己定义一个名字) 查看正在运行的servicedocker service ls 查看每个容器进程的process iddocker service ps service_id docker inspect container_name(通过docker container ls可以查看) docker inspect –format{} container_name 来限制输出的内容 scale利用swarm进行伸缩 直接修改docker-compose.yml文件中的replicas的个数然后重启docker： docker stack deploy -c docker-compose.yml appname不需要停止原来的app，热更新 停止stackdocker stack rm appname 删除了service，但是container还在docker swarm leave –force 删除container Swarms:多台机器(实体机或者虚拟机)有一台机器作为swarm manager,所有的命令都在swarm manager上运行Swarm managers are the only machines in a swarm that can execute your commands, or authorize other machines to join the swarm as workers. Workers are just there to provide capacity and do not have the authority to tell any other machine what it can and cannot do. 在swarm manager上运行docker swarm init（多张网卡的话：docker * swarm init –advertise-addr 10.239.182.67）使能swarm模式。在其它节点的机器上面运行docker swarm join使其它机器加入swarm作为workersdocker swarm init 会返回告诉你docker swarm join的命令里面的token怎么敲 在其它机器上运行docker swarm join，让其它机器加入如果没办法加入，首先确保关闭manager的防火墙：http://www.jianshu.com/p/3ab849248b52 docker的各个端口的作用：https://www.digitalocean.com/community/tutorials/how-to-configure-the-linux-firewall-for-docker-swarm-on-centos-7 在公司电脑上是有可能是代理的问题：https://github.com/moby/moby/issues/34825 在/etc/systemd/system/docker.service.d/[Service]Environment=”http_proxy=http://child-prc.intel.com:913“ “NO_PROXY=localhost,127.0.0.1,10.239.182.67”把manager的地址添加到no_proxy里面 在manager上运行docker node ls查看机器个数 stack:A stack is a group of interrelated services that share dependencies, and can be orchestrated and scaled together. 在docker-compose.yml文件的services的tag下面再去定义一个service多个service一起部署 Kubernetes作用和docker swarm类似（解决容器编排问题），但是比swarm更强大 基本概念 pod：容器的集合，一个pod可以包含一个或多个container。一个pod内运行相同业务的容器，一个pod只能运行在一台机器上 replicateion controller(rc) 管理pod，保证任何时间有特定数量的pod在运行，多了删除pod，少了创建pod service：将pod提供的服务暴露到外网 label： 用于区分 pod,rc,service的key/value pair kubectl: 命令行,调用kubernetes的API Kubernetes master: 运行在主节点上，收集三个进程的信息：: kube-apiserver, kube-controller-manager and kube-scheduler 其它非主节点运行两个进程：kubelet(与主节点通信), kube-proxy(网络代理，反映了各个节点的kubernetes的网络状况) Kubernetes Control Planek8s里面所有组件都叫做对象，Kubernetes Control Plane在任意时间都会监测这些对象，保证对象的数量和目标状态 etcd 是 CoreOS 团队发起的一个管理配置信息和服务发现（service discovery）的项目http://www.infoq.com/cn/articles/coreos-analyse-etcd install我们是在cluster上装：https://kubernetes.io/docs/getting-started-guides/centos/centos_manual_config/这个文档不能用了，有错问题1：当前k8s还不支持docker17版本需要用docker1.12版本，https://stackoverflow.com/questions/44891775/kubernetes-installation-on-centos7 所以需要首先卸载docker17版本，直接用k8s的安装手册，上面会安装合适版本的docker老版本的docker的命令和新版本不太一样 现在推荐用kubeadm去enable，过程和swarm有点像https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/http://tonybai.com/2016/12/30/install-kubernetes-on-ubuntu-with-kubeadm/ 比较容器调度方案docker swarm VS k8shttp://dockone.io/article/1138http://www.jianshu.com/p/07daa3a16878]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习知识点]]></title>
    <url>%2F2017%2F09%2F13%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[玻尔兹曼机 过拟合：偏差很小，但方差很大（所以欠拟合叫做高偏差，过拟合叫做高方差）训练数据太少，模型参数太多解决方法：1. 剔除不必要的模型参数 2. 使用正则化（regularizaion），保留模型参数，但是减小模型参数的数量级（对某些参数，在loss函数中添加惩罚项） L2正则项解决过拟合的问题https://www.zhihu.com/question/20924039 交叉验证https://baike.baidu.com/item/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81将数据集分成多个多个小的数据集，先用小数据集1训练，用小数据集2验证（评估正则化函数），直到最后一个小的数据集被用于验证 极大似然估计http://www.jianshu.com/p/f1d3906e4a3e 激活函数在神经网络中引入非线性的成分sigmod：0，1之间tanh：-1，1之间softmax: 多分类，一般用在输出层的激活 epoch、 iteration和batchsizehttp://blog.csdn.net/sinat_30071459/article/details/50721565 交叉熵https://zhuanlan.zhihu.com/p/27223959在机器学习中，可以用交叉熵来定义loss function BP神经网络反向传播计算——BackPropagation单个参数：http://www.cnblogs.com/charlotte77/p/5629865.html矩阵计算：http://www.jeyzhang.com/cnn-learning-notes-2.html 卷积神经网络CNN局部连接，参数共享，卷积，池化等概念介绍：http://www.jeyzhang.com/cnn-learning-notes-1.html模型训练：http://www.jeyzhang.com/cnn-learning-notes-2.html 局部对比归一化Local contrast normalizationhttp://blog.csdn.net/zouxy09/article/details/10007237 batch normalizationhttp://blog.csdn.net/hjimce/article/details/50866313http://minibatch.net/2017/06/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87-Batch-Normalization/https://zhuanlan.zhihu.com/p/26682707反向计算梯度：http://mlnote.com/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/ Good:https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html 白化http://blog.csdn.net/hjimce/article/details/50864602http://blog.csdn.net/whiteinblue/article/details/36171233]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab]]></title>
    <url>%2F2017%2F09%2F10%2Fcrontab%2F</url>
    <content type="text"><![CDATA[介绍crontab常用来周期性的执行命令。 启动service cron(crond) status //ubuntu 和centos有不同 service cron(crond) start(restart,reload) //如果没有启动，启动一下 用法查看crontab命令：-l``` :系统级配置的任务好像无法通过crontab -l 来查看12编辑crontab命令:```crontab -e 移除crontab命令:-r```12345### 修改配置文件系统级配置:在/etc/crontab 文件中编辑用户级配置:用特定用户登录，在命令行中配置命令，是配置和用户相关的```crontab -e 将命令写入文件中，然后直接crontab filename来读取文件中的所有命令 配置文件的写法： 周期性定时执行 分 小时 日 月 星期 命令 0-59 0-23 1-31 1-12 0-6 command (取值范围,0表示周日一般一行对应一个任务) 日中有数字，表示的每个月到这一日都会执行月中有数字，表示每年到这个月都会执行星期中有数组，表示每周到这一天都会执行 在特定时候执行 @reboot commandsuch as：@reboot root /bin/bash /test.sh debug查看/var/log/cron文件]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建和发布npm包]]></title>
    <url>%2F2017%2F08%2F05%2F%E5%88%9B%E5%BB%BA%E5%92%8C%E5%8F%91%E5%B8%83npm%E5%8C%85%2F</url>
    <content type="text"><![CDATA[创建包123//在初始化话时我们定义了module的入口文件是index.js//在初始化的时候输入的package的名字就是后面npm install时的名字，后面也可以再package.json里面修改npm init 在index.js中定义需要export出来的函数 发布包新用户,先去npm的网站注册 12345678//输入注册npm时的账号密码以及邮箱npm adduser//查看信息npm config ls//发布npm publish 更新包修改的原有的包的代码之后不可以直接publish的已有的仓库和版本当中,会提示我们没有权限复写已经publish的包这时候需要修改package.json里面包的版本号 在已经依赖于这个的包的用户这边1npm update 可以将使用的包更新的最新 包的使用需要使用这个package的时候1npm install pkg-name --save(--save-dev) 删除已经发布的包1npm unpublish [&lt;@scope&gt;/]&lt;pkg&gt;[@&lt;version&gt;] 在Git上维护自己的包新建一个主分支和一个开发分支每一部分开发完成开一个版本分支，将版本的分支publish到npm上面 在develop分支上进行开发，开发完成切换到对应的版本的备选分支，review备选分支的代码，没有问题的话merge的备选分支里面修改package.json里面的版本号，上传到git以及发布到npm切回到develop分支进行新的开发]]></content>
      <tags>
        <tag>技术 Node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于express和reactjs开发的web框架]]></title>
    <url>%2F2017%2F07%2F23%2F%E5%9F%BA%E4%BA%8Eexpress%E5%92%8Creactjs%E5%BC%80%E5%8F%91%E7%9A%84web%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[前几个月基于express和reactjs写过几个简单的web应用，现在回过头来看很多代码都想不起来了。这周利用下班时间重新将这个框架梳理了一下，并写下这篇文章作为基于这两个框架开发web应用的BKM。 使用到的主要工具介绍 Express：基于nodejs的后端框架 Reactjs+Redux：前端框架，React更多的是扮演着MVC中的view的角色，Model以及Control类似的功能可以用flux或者更进一步用Redux来实现，链接 里面的图片将redux和reactjs之间的关系描述的很清楚 Babel：转换ES6以及JSX的语法到ES5，commonJS Webpack: 浏览器不支持commonJS，因此需要通过webpack打包之后，才可以将js嵌入到html当中，当然经过webpack之后在生产环境中最好再用gulp经过一次uglify，可以减小js文件的大小 Gulp：前端开发自动化的利器，在开发过程中可以监听文件的变化重启服务器 数据库：目前使用MySQL存储用户信息，使用Redis存储session的hotdata。下一步可以把mongodb的接口移植过来。 框架实现的基础功能这是个基础框架，不涉及任何的业务逻辑。在这个框架的基础上可以根据实际需求，开发主页面和相关的业务逻辑。 注册和登入功能：使用bcrypt模块对密码进行加密，用户名和密码存储在MySQL里面进行用户信息的验证。验证通过、用户登入之后在cookie中设置session id对应的session数据，并将session的用户热点数据储存在Redis里面。 登出功能：通过销毁和重新建立session，来实现用户的登出功能。redis的session数据可以再redis里面设置数据自动销毁的超时时间。当然cookie中的session id也可以通过设置cookie的超时销毁时间来实现。 用户登入状态控制：用户的每一次访问根据cookie中的session id进行用户登录状态的控制，未登录用户无法看到定制的信息。 业务逻辑：根据业务需求在main页面中进行业务逻辑的开发 Reactjs + Redux的前端框架介绍框架的介绍这里介绍一下我在写这个框架的时候的一些BKM： 一般在javascript目录下面建立三个目录：一个react目录，是开发的前端代码；一个babel目录：是将react目录下代码经过babel转译之后得到的代码;一个webpack目录：是将babel目录下需要嵌入在html中的代码打包之后的代码。 其中只在react目录下开发我们的代码：react的根目录下面：有一个store.js文件，这个是在redux中存储应用状态和数据的。除了store.js之外的文件都是component文件，就是html页面的显示元件，component文件可以通过container目录下面的各个container文件包含的模块化的组件拼凑出来，因此我们说reactjs很好的做到了前端组件的模块化和复用。 react目录下面的container目录包含了就是前端模块化的组件。react目录下面的action目录下的代码功能类似于MVC模型中的control，就是一些前端的触发函数，在这些函数被执行之后可以dispatch一些action，reducer就在监听这些action，并获得action的返回数据 react目录下面的reducer目录下的代码，就在监听action目录下代码派发的action，一旦监听到对应的代码，就可以触发页面跳转或者返回状态数据到store当中，这些数据通过store和container绑定的，就会触发container中对应的数据的更新。每一个reducer的作用就是传入上一次的state和action，输出这一次的新的state的对象。export var logout = function(state = {logout:null},action)，这里面state = {logout:null}，只有当第一次调用到的时候才会有{logout:null}，以后每一次调用到都会输入上一次的state对象。 一些常见的问题react：component的名字要大写开头redux： reducer下面每个case里面返回的对象每一次都需要新建，可以用object.assign(用在单个对象添加属性可以返回新的对象)，用filter也会新建对象(可以用在数组上面)，redux如何获取服务器端数据库的数据，在action里面写restful的请求给服务器端，需要配置多个action，一个action1发送请求，在action1获得请求的数据处理中dispatch一个数据完成的action 使用redux实现撤销undo和再做redo功能基本的思想，就是在原有的reducer的基础上，新建一个高阶的reducer（什么是高阶：就是指函数返回的值也是函数）参考这部分代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364export var saveCurrentCommentState = function(state = &#123;currentComment:null&#125;,action)&#123; switch (action.type) &#123; case &apos;SAVE_COMMENT&apos;: console.log(&apos;return the saveCurrentComment&apos;,state, &apos;and action&apos;, action); //console.log(action.payload.username); //console.log(Object.assign(&#123;&#125;,state,&#123;username:action.payload.username&#125;)); return Object.assign(&#123;&#125;,state,&#123;currentComment:action.payload.comment&#125;); default: console.log(&apos;return the defalut saveCurrentComment&apos;,state, &apos;and action&apos;, action); return Object.assign(&#123;&#125;,state); &#125;&#125;;var highOrdersaveCurrentCommentState = function(reducer)&#123;//for pastComment currentComment and futureComment: each one is a state or state array const initialState = &#123; pastComment: [], currentComment: reducer(&#123;currentComment:null&#125;, &#123;&#125;), futureComment: [] &#125;; return function (state = initialState, action) &#123; const &#123; pastComment, currentComment, futureComment &#125; = state; switch (action.type) &#123; case &apos;UNDO_COMMENT&apos;: console.log(&apos;return the highOrdersaveCurrentCommentState&apos;, state, &apos;and action&apos;, action); //console.log(action.payload.username); //console.log(Object.assign(&#123;&#125;,state,&#123;username:action.payload.username&#125;)); const newpastComment = pastComment.slice(0, pastComment.length - 1); const previousComment = pastComment[pastComment.length - 1]; futureComment.push(currentComment); console.log(&quot;=========&gt;&quot;); console.log(previousComment); return &#123; pastComment: newpastComment, currentComment: previousComment, futureComment: futureComment &#125;; //return Object.assign(&#123;&#125;, state, &#123;comment: action.payload.comment&#125;); case &apos;REDO_COMMENT&apos;: console.log(&apos;return the highOrdersaveCurrentCommentState&apos;, state, &apos;and action&apos;, action); pastComment.push(currentComment); const nextComment = futureComment[futureComment.length - 1]; const newfutureComment = futureComment.slice(0, futureComment.length - 1); return &#123; pastComment: pastComment, currentComment: nextComment, futureComment: newfutureComment &#125;; default: console.log(&apos;return the defalut highOrdersaveCurrentCommentState&apos;, state, &apos;and action&apos;, action); const newcurrentComment = reducer(currentComment, action); if (newcurrentComment === currentComment) &#123; return state &#125; pastComment.push(currentComment); return &#123; pastComment: pastComment, currentComment: newcurrentComment, futureComment: [] &#125; &#125; &#125;&#125;;export var undoRedoCommentState = highOrdersaveCurrentCommentState(saveCurrentCommentState); 其中saveCurrentCommentState这个reducer是一个基本的reducer，每一次正常的状态更新都会触发这个reducer来更新对应的状态定义一个高阶函数highOrdersaveCurrentCommentState，这个函数传入的参数是一个reducer(In)，返回的也是一个reducer(Out)。定义一个新的reducer，undoRedoCommentState，这个reducer就是通过调用 highOrdersaveCurrentCommentState(saveCurrentCommentState)得到的返回的reducer。redo和undo的功能都是在这个highOrdersaveCurrentCommentState高阶函数里面实现的，我们现在来看看这个函数里面实现了什么。这个函数只在初始化的时候被调用了一次，首先定义reducer(Out)的初始化的initialState ，并传入到返回的reducer中。在返回的这个reducer中： 如果没有监测到undo或者redo的action，就会触发default的操作，在default操作中会先调用saveCurrentCommentState这个基本的reducer看看是否有正常的状态转移（也就是判断一下是不是状态转移的action触发的，还是和当前的reducer没有半毛钱关系的action触发的），如果是正常的状态转移，那么当前的state加入到past里面，更新当前的state。 如果监测到undo的action，就会把当前的state放入到future中，从past里面取一个state出来放到当前的state里面 如果监测到redo的action，就把当前的state放入到past里面，从future里面取状态回来。 主要的实现逻辑就是这些。当然在这部分代码里面还可以进一步添加，undo的时候是不是past里面存有过去的状态（即是不是完全popout的），redo的时候也需要看看future里面的状态是不是完全popout的。 代码的链接github托管的代码的链接地址其中相对于master分支，develop分支实现了用户数据在session中的储存，用户的登出功能。 下一步开发计划 用户登入部分改成https的请求]]></content>
      <tags>
        <tag>技术 web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python调用命令行脚本]]></title>
    <url>%2F2017%2F07%2F19%2Fpython%E8%B0%83%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[阻塞态调用和非阻塞态调用，这两个名字是自己根据调用的特点给区分的。两者主要的区别在于是否会另外开辟一个子进程去调用这些命令行的脚本。在所谓的阻塞态调用下，python会等待这个脚本执行完毕再顺序往下执行其它的程序。在所谓的非阻塞态调用下，python则会开辟一个子进程，将脚本放在子进程里面执行，自己则立刻向下运行。 阻塞态调用使用os模块，主要涉及到popen和system两种方法。这两种方法的区别参考一下链接os.system(cmd) 返回的是程序运行的结果状态，比如程序运行成功，则返回0，有错误则返回其对应的错误代码os.popen(cmd) 返回的是程序输出的结果，比如cmd=“ls”的时候，通过popen就可以得到ls的结果 123cmd = &quot;echo hello wolrd&quot;ret = os.popen(cmd)information = os.system(cmd) 非阻塞态调用非阻塞态调用主要用到了subprocess 这个模块简单的用法就用subprocess.call(cmd)就可以了更高级复杂的用法可以使用subprocess.popen(cmd)比如说有时候因为环境变量的配置我们需要在特点的目录下执行脚本，就可以使用subprocess.popen的cwd参数,要配置环境变量而不是继承原有的环境变量可以使用env参数123workingDirector = r&quot;C:/testcode&quot;cmd = r&quot;test.bat&quot;subprocess.popen(cmd,cwd=workingDirector)]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络（CNN）]]></title>
    <url>%2F2017%2F07%2F15%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[介绍 卷积核： 用来和图像做卷积运算的小矩阵 stride 步长：每一次卷积运算之后，卷积核沿着图像移动的像素点的长度 Padding： 在图像边沿位置做卷积运算时，需要补充的像素点，一般像素点值补0就可以 步骤 图像卷积（将特征值平均化，提取特征值）,根据需要 可以/不 将图像padding补0，定义一个需要的卷积核(如果定义多个不同的卷积核，每一个卷积核可以对应于一个图像的通道(ex. rgb/hsv 三个通道)，卷积和池化之后就可以得到不同的通道对应的新的图像 池化（降采样，减少图像运算的像素点的个数），con-max-pooling 就是取一片区域当中的最大值，对每一个通道都进行池化操作1，2步骤可以重复多次最后将图像的每一个通道都flattern（展开成一维数组），每一个通道都作为一个输入量，输入到全连接的前馈神经网络中进行训练（一般在网络中会对不同通道的值进行融合） CNN用于手写数字识别整个项目的框架如下图所示：通过摄像头捕获手写数字，输入到计算机中，运行手写数字算法，将识别结果通过无线设备发送到智能硬件算法的框图入如下图所示整个神经网络共包含了5层（除去第一层输入层）其中包含了二层卷积层、一层全连接层、一层dropout层以及一层的输出层在每一层的卷积层中，每一个卷积操作，定义stride为1，Padding补充为0；每一个池化操作，都使用2x2的矩阵块中取出最大值 第一层神经网络第一个卷积层定义了32个通道（对应了32个卷积核），每一个卷积核是5x5的矩阵块1234# First convolutional layer - maps one grayscale image to 32 feature maps.W_conv1 = weight_variable([5, 5, 1, 32])b_conv1 = bias_variable([32])h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) 其中weight_variable([5, 5, 1, 32])前两个参数对应了卷积核的大小，第三个参数是输入的通道数(原始为灰度图像1个通道)，第四个参数为输出的通道数最后用卷积后的结果作为relu神经元的构造函数，神经元里面定义了对应的激活函数 第一个池化层1h_pool1 = max_pool_2x2(h_conv1) 第一个卷积层输出为32个通道，每一个通道的图像是28x28经过一个2x2的池化层之后，输出的每一个通道的图像是14x14 第二层神经网络第二个卷积层定义了64个通道（对应了64个卷积核），每一个卷积核是5x5的矩阵块123W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) 输入为32个通道每个通道为14x14的图像经过卷积运算，输出为64个通道每个通道14x14的图像1h_pool2 = max_pool_2x2(h_conv2) 第二个池化层输入为64个通道每个通道14x14的图像输出为64个通道每个通道7x7的图像 第三层神经网络经过前两层已经完成了卷积运算，特征提取和降维第三层叫做全连接层fully connected layer 也叫做 Densely Connected Layer第二层的输出为64通道，每个通道为7x7的图像所以先将第二层的输出flattern1h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) 第三层我们定了了1024个神经元所以做乘法需要1024（77*64）个权重的参数以及1024个bias偏差量1234W_fc1 = weight_variable([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) 最后构造成relu神经元 Dropout层为了避免过拟合添加了Dropout层dropout的概念：就是在训练时，对每一批次的训练数据，关闭部分神经元，这样每一批次的数据只训练了部分神经元的参数，最后把这些参数累加并乘以一个系数得到训练后模型的参数12keep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 通过keep_prob参数来控制需要关闭的神经元的比例 输出层经过第三层之后输出为1024shape的数据我们希望得到的属于10个数据的证据，可以将证据转为为概率所以这一层的输出为10维的向量1234W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 代码链接最后附上训练好的模型、图像预处理代码以及测试集的下载地址 资料http://www.jeyzhang.com/cnn-learning-notes-1.htmlhttp://www.jeyzhang.com/cnn-learning-notes-2.html]]></content>
      <tags>
        <tag>技术 深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手写数字识别]]></title>
    <url>%2F2017%2F07%2F06%2F%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言这几日，作息回到了早睡早醒，每天就花些时间去读书、去摄影、去锻炼以及做那些在WaitingList里躺了很久的有趣的事。老板出差在美国，工作显得并不是那么忙碌，就在每天下班后抽出几个小时的时间去做了个手写数字识别的项目。主要涉及的技术包括了：机器视觉、深度学习（人工神经网络），当然相关的编程语言也是不可少的。 深度学习就像hello world是每一个码农写的第一行代码，手写数字识别也是每一个想进入人工智能领域的人必上的第一课。深度学习的框架很多，比如我司的caffe，但是由于最近实在对C++无感，所以本文采用了tensorflow，这个google大法开源的python框架。如果官网访问有难度，请自觉科学上网。训练的数据集来自Yann LeCun’s website训练模型我采用了最简单的softmax，因为急于先将整个流程走通，这个模型最后的测试集合的识别率只有92%。很槽糕，对不对，但是没关系以后还可以慢慢优化，:)希望可以说到做到。训练过程用了梯度下降法去优化模型参数，大约300圈，识别率就打到了90%左右，后面的迭代基本就是不断的出现过拟合和修正过拟合问题。在训练结束之后可以保存训练模型：123saver = tf.train.Saver()save_path = &quot;./model/model.ckpt&quot;saver.save(sess, save_path) 以后在inference阶段，就可以直接提出这个模型和训练好的参数123saver = tf.train.Saver([W, b])sess = tf.Session()saver.restore(sess, &quot;./model/model.ckpt&quot;) 有没有觉得这个过程很简单，哈哈，那是因为我对这个算法也是一知半解，也失去了少年时推导出数学公式时的欣喜。 图形预处理事实上，更多的时间花会花在inference阶段如何对识别到的图形进行预处理上面。机器视觉最主要的一个问题在于算法的通用性上，因为视觉受环境的影响很大，不同光照环境中得到的图片差别就像隔了一条银河。所以你会发现在自动驾驶领域，总会集成视觉、激光雷达、IMU等多种类型的传感器(ps.多元传感器信息融合是我的老本行，哈哈，爸爸就是靠这个硕士毕业的)。所以在拍摄到每一张图片以后就需要对图片进行预处理和特征的提取，这里只罗列了关键的步骤，详细的代码请参考附录我的git的仓库。 拍摄得到原始彩色图片 做一次5阶的gauss平滑 转换成灰度图像 二值化，二值化的阈值要考实验，当然参数不好后面也有一步去补救 对二值化后的图片进行腐蚀和膨胀，主要是去除噪点让图像看起来比较饱满 用canny算子提取ROI(region of interest 不知道中文怎么说，感兴趣的区域？)的轮廓 找到ROI的外接矩形 裁剪出ROI的subImage resize 这个subImage倒特定的像素点（因为训练模型需要固定像素点的输入图形） 最后再对这张图做一次膨胀和腐蚀（注意顺序和前面不一样哦，哈哈，想知道为什么请自行谷歌） 最后将这张图形输入inference，模型就可以告诉你你到底写的是什么鸟语了 TensorBoard图形化展示在模型的最后添加一行代码保存模型图到文件，这里保存到了当前.路径下面1file_writer = tf.summary.FileWriter(&apos;.&apos;, sess.graph) 之后只需要在对应的路径(这里为当前路径)下运行1tensorboard --logdir . 之后就可以显示出图形化展示的URL地址。ex.http://192.168.1.6:6006直接访问以上地址就可以读取训练模型相关的图形化的显示,详细的资料可以参考一下链接 下一步计划 搭建更好的训练模型 改善预处理的算法 做一块智能硬件，和电脑端无线通讯，在电脑端得到识别结果之后，发给硬件板子，板子可以触发对应的动作，比如说控制无人机的姿态，想不出来其它点子，有点累了，脑洞变小了。 附录最后附上训练好的模型、图像预处理代码以及测试集的下载地址]]></content>
      <tags>
        <tag>技术 深度学习 图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python类与继承]]></title>
    <url>%2F2017%2F06%2F27%2FPython%E7%B1%BB%E4%B8%8E%E7%BB%A7%E6%89%BF%2F</url>
    <content type="text"><![CDATA[类变量与实例变量看这个例子因为当用self.var去call一个变量的时候的顺序是实例的dict-&gt;类的dict-&gt;基类也就是如果再实例里面找到这个变量相同的名字，就用实例的变量，没有找到就去找类变量，再没有找到就会找基类的变量，最后还没有找到就会报错类变量还可以用 类名.变量名去调用实例变量只可以用self.变量名 类的实例方法，静态方法与类方法参考这个链接 实例方法形式：class A(object):def function(self,var1):最常见的方法包括了init等函数 类方法用@classmethod修饰，需要传入一个非self参数，这个参数的名字常写作cls或者cls_obj(也可以是其它名字，不是self), 可以被实例调用也可以被对象调用 1234567891011121314class Person(object): num = 0 def __init__(self,name): self.name=name Person.num +=1 @classmethod def get_nomber_of_instance(cls): return cls.numif __name__ == &quot;__main__&quot;: print(&quot;hello world&quot;) a = Person(&quot;bob&quot;) b = Person(&quot;bob2&quot;) print(Person.get_nomber_of_instance()) print(a.get_nomber_of_instance()) 这样的好处是在类的内部，通过第一个参数cls把类传递出来 静态方法用@staticmethod修饰，它的适用场景决定了它一般不需要传入参数(self也不需要传入)12345678910111213141516171819202122232425#needPrintName = &quot;no&quot; #yes or noneedPrintName = &quot;yes&quot;class Person(object): num = 0 def __init__(self,name): self.name=name Person.num +=1 def printName(self): if self.needPrintName(): print(self.name) else: print(&quot;Doesn&apos;t need to print name&quot;) @classmethod def get_nomber_of_instance(cls): return cls.num @staticmethod def needPrintName(): return needPrintName==&quot;yes&quot;if __name__ == &quot;__main__&quot;: print(&quot;hello world&quot;) a = Person(&quot;bob&quot;) b = Person(&quot;bob2&quot;) print(Person.get_nomber_of_instance()) print(a.get_nomber_of_instance()) a.printName() 这样的好处是，有一些方法和类的功能相关，但是又不需要类或者对象本身去参与 类的继承最基础的类就是objectclass child_class(base_class):注意点： 派生类不会自动调用基类的init方法，需要在派生类的init函数里面去调用，base_class.init(self) 调用基类的方法的时候，需要加上基类的类名前缀，而且调用基类的函数记得带上self 总是先在本类中找方法，找不到再去基类里面找（派生类的方法会覆盖基类的方法）123456789101112131415161718192021222324class Person(object): num = 0 def __init__(self,name): self.name=name Person.num +=1 def printName(self): print(self.name) @classmethod def get_nomber_of_instance(cls): return cls.numclass Student(Person): def __init__(self,name,score): Person.__init__(self,name) self.score=scoreif __name__ == &quot;__main__&quot;: print(&quot;hello world&quot;) a = Person(&quot;bob&quot;) b = Person(&quot;bob2&quot;) print(Person.get_nomber_of_instance()) print(a.get_nomber_of_instance()) a.printName() b.printName() c = Student(&quot;bob3&quot;,100) c.printName() 如果基类继承了object类，还可以用super去调用基类的方法推荐这种写法super(chrild_class,self)._init__(arg)(这里的arg不写self)，因为原来直接call基类的init函数存在这样的一种例外的情况1234567891011121314151617181920212223242526import sysclass Person(object): num = 0 def __init__(self,name): self.name=name Person.num +=1 def printName(self): print(self.name) @classmethod def get_nomber_of_instance(cls): return cls.numclass Student(Person): def __init__(self,name,score): super(self.__class__,self).__init__(name) self.score=scoreif __name__ == &quot;__main__&quot;: print(&apos;all argv:&#123;&#125;&apos;.format(sys.argv)) print(len(sys.argv)) a = Person(&quot;bob&quot;) b = Person(&quot;bob2&quot;) print(Person.get_nomber_of_instance()) print(a.get_nomber_of_instance()) a.printName() b.printName() c = Student(&quot;bob3&quot;,100) c.printName() 备注 获取对象的类 ins_obj.class.method(var)123456789101112def howperson(a): return a.__class__.numclass Person(object): num = 0 def __init__(self,name): self.name=name Person.num +=1if __name__ == &quot;__main__&quot;: print(&quot;hello world&quot;) a = Person(&quot;bob&quot;) b = Person(&quot;bob2&quot;) print(howperson(a))]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bash学习笔记]]></title>
    <url>%2F2017%2F06%2F12%2Fbash%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[diffdiff 用于比较两个文件（也可以用于目录）的差别1diff file_1_1.txt file_1_2.txt 常规模式输出： a为后一个文件比前一个文件多的行 d为后一个文件比前一个文件少的行 c为两个文件不一样的行 1diff file_1_1.txt file_1_2.txt -y 比较模式输出：按每一行对比两个文件 vim查找字符串输入查找在命令模式下输入 ？或者/然后输入要查找的单词按回车开始查找，按n查找下一个(上一个)单词N与n的反方向进行搜索 根据光标位置的单词查找在光标位置的单词按*或者#按n查找下一个(上一个)单词，N与n的反方向进行搜索 后台运行Mthod1: ./cycle.sh 200 chassis31_6 AC 2&gt;&amp;1|tee chassis31_6.log &amp;在命令的最后加一个&amp;，表示希望这个命令在后台运行Method2: 使用screen $?$? 表示上一个进程或者函数运行之后的返回值在写bash函数的时候，函数里面return返回值在调用这个函数之后再调用一次 $? 表示读取一次返回值，注意$?只能用一次 重定向&gt; /dev/null 2&gt;&amp;1命令 &gt; /dev/null 2&gt;&amp;1命令 1&gt; /dev/null 2&gt;&amp;1两条都不会输出任何东西，表示将错误定向到标准输出，又将标准输出定向到null这里的&amp;表示后面的1是文件描述符（标准输出），并不是文件名 运行脚本`` `command`键盘最左上角的字符，也就是markdown里面的代码的符号表示运行``之间的command grep匹配行首grep &quot;^${group}&quot; local.cfg | awk -F: &#39;{print $2}&#39;^表示匹配行首 匹配正则表达式 =~[[ $phrase =~ $keyword ]]正则表达式 bash数组申明：declare -a arr12341. $&#123;arr[*]&#125; # All of the items in the array2. $&#123;!arr[*]&#125; # All of the indexes in the array3. $&#123;#arr[*]&#125; # Number of items in the array4. $&#123;#arr[0]&#125; # Length of item zero 构造数组arr=(element1 element2 … element3) 为数组添加新的元素arr=(${arr[@]} elementNew)for example: chassisarr=(${chassisarr[@]} $group)意思是，取出原有数组中的所有元素加上新元素赋值回数组 Top命令http://www.cnblogs.com/peida/archive/2012/12/24/2831353.html 查看进程包含的线程的信息首先top看到进程的pid然后:top -p PID -Hd1]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[powerman]]></title>
    <url>%2F2017%2F06%2F10%2Fpowerman%2F</url>
    <content type="text"><![CDATA[介绍powerman是一个开源的工具，被谷歌用于cluster的机群管理通过SNMP协议可以管理PDU电源通过ipmi协议可以管理机器的reboot以及开关机 安装源码下载地址安装流程参考源码中的INSTALL文件 configure自动生成makefile 1./configure 编译 12makemake check //check the make result 安装 1make install]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning]]></title>
    <url>%2F2017%2F05%2F20%2FDeepLearning%2F</url>
    <content type="text"><![CDATA[框架概述 Tensorflow： Google Python Caffe： well supported by Intel Synaptic：Nodejs Tensorflow]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用别名远程登入服务器]]></title>
    <url>%2F2017%2F05%2F20%2F%E4%BD%BF%E7%94%A8%E5%88%AB%E5%90%8D%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%85%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言远程登入服务器进行维护的时候，SSH登入有两种模式ssh username@ip 模式1：使用密码登入，输入username的配置密码 模式2：使用密钥登入，在本地机器使用ssh-keygen生成私钥与公钥，将公钥放置到服务器上，以后就可以免密码登入了使用 模式2 ssh username@ip登入服务器时，经常会忘记ip地址，因此更好的方法是配置服务器的别名alias，以后使用ssh alias 就可以登入服务器了 使用别名登入 打开本地机器的~/.ssh/config文件，添加以下内容：123456Host server-alias # server-alias为希望配置的服务器别名HostName server-ip # 服务器地址Port 22User username # 服务器端用户名PreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsa # 私钥地址，默认为 ~/.ssh/id_rsa 配置之后就可以通过别名远程登入服务器了1ssh server-alias 上述方法通过自己的服务器配置验证]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CherryPy]]></title>
    <url>%2F2017%2F05%2F12%2FCherryPy%2F</url>
    <content type="text"><![CDATA[介绍CherryPy是一个python的web框架是目前接触过的三种python编写的web框架里(Django,Tornado,CherryPy)最轻量化的因为公司项目的开发需要，最近开始用每天的业余时间学习一下这个框架设计模式首先谈一下读完教程配合着写一些简单的应用之后的整体感受，相比于另外两个框架，CherryPy配合着SQLite使用的确做到了轻量化，但是这也意味着框架固定，灵活性不足。 安装123pip install cherrypy#使用import cherrypy 框架搭建最小应用时可以参考我的仓库 配置文件定义一个字典通常命名为conf，可以定义多个不同的配置字典conf1，conf2 启动服务有两种方式：1.1cherrypy.quickstart(webapp, &apos;/&apos;, conf) webapp是类名，在这个类中定义RESTAPI‘/’是对应的URLconf是对应的配置文件2.1234cherrypy.tree.mount(webapp, &apos;/&apos;, conf)#cherrypy.tree.mount(Forum(), &apos;/forum&apos;, forum_conf)cherrypy.engine.start()cherrypy.engine.block() 渲染文件在根目录定义index.html文件然后定义入口 1234class StringGenerator(object): @cherrypy.expose def index(self): return open(&apos;index.html&apos;) RESTFUL API1234567891011@cherrypy.exposeclass myFirstService(object): @cherrypy.tools.accept(media=&apos;text/plain&apos;) def GET(self): with sqlite3.connect(DB_STRING) as c: cherrypy.session[&apos;ts&apos;] = time.time() r = c.execute(&quot;SELECT * FROM STUDENT&quot;) print r.fetchone() return &apos;hhui&apos; def POST(self): 静态文件在配置中添加1234&apos;/static&apos;: &#123; &apos;tools.staticdir.on&apos;: True, &apos;tools.staticdir.dir&apos;: &apos;./public&apos;&#125; 所以根目录下的public文件夹里的东西对应了就是URL-static 数据库CherryPy配合SQLite使用可以搭建轻量化的web应用通常在Linux发行版本中都会预装SQLite的数据库定义DB文件的名字DB_STRING = “testDB.db”连接数据库||执行CURD操作12345with sqlite3.connect(DB_STRING) as c:cherrypy.session[&apos;ts&apos;] = time.time()r = c.execute(&quot;SELECT * FROM STUDENT&quot;)print r.fetchone()return &apos;hhui&apos;]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重定向]]></title>
    <url>%2F2017%2F05%2F10%2F%E9%87%8D%E5%AE%9A%E5%90%91%2F</url>
    <content type="text"><![CDATA[介绍重定向是linux很常用的命令 cmd &gt; record.txt 将cmd的输出从命令行重定向到文件中，先将文件清除再写 cmd &gt;&gt; record.txt 和&gt;类似，但是是追加的模式 文件描述符文件描述符 是与打开的某个文件或者数据流相关联的整数。文件描述符0,1,2是系统预留的。 0 - stdin(标准输入) 1 - stdout(标准输出) 2 - stderr(标准错误) 2> record.txt 只将错误信息重定向的文件中，**正常的信息输出在命令行**123456789也可以这样：cmd 2&gt; out.txt 1&gt; temp.txt将 **标准错误和标准输出** 重定向到两个文件中。当然，还有更精简的方式输出到同一个文件中：cmd 2&gt;&amp;1 out.txt进一步这条命令可以简写为cmd &amp;&gt; out.txt或者cmd &gt;&amp; out.txt]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow学习笔记]]></title>
    <url>%2F2017%2F05%2F06%2Ftensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[前言Tensorflow 是Google出的基于人工神经网络(ANN)的深度学习的框架，官网需要科学上网工具才能访问 安装 为了避免python环境的污染，可以先安装一个python的虚拟环境VirtualEnv 打开虚拟环境，用pip安装tensorflow的包 和普通包一样使用tensorflow的包 硬件有GPU别忘了使用CUDA 实验源代码用的是anishathalye大神的仓库先看看最后训练出来的效果图 原图是去年在上海迪士尼玩的时候随手拍的照片 想要训练的风格图片用的梵高的 《星夜》 结论 玩机器学习对计算机的硬件要求很高，这是GPU的强项下面这个实验在一台cpu为i5-4200H，8G内存的台式机上跑的结果，整个训练过程时长了5个小时，运行过程中使用TOP命令查看，CPU一直处于100%以上的负荷，如果有GPU的话，理论上时间应该再20分钟左右结论 没有GPU硬件的支持是玩不了机器学习的]]></content>
      <tags>
        <tag>技术 tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Awk学习笔记]]></title>
    <url>%2F2017%2F05%2F04%2FAwk%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[基本用法 awk ‘{print $0}’ filename 打印第n，m个字段 awk ‘{print $n $m}’ filename 添加上说明 awk ‘{print “字段n” $n “字段m” $m}’ filename 设置分隔符awk -F”:” ‘{print $0}’ filename 编程文件test.awk文件123456789BEGIN&#123; FS=&quot;:&quot;&#125;&#123; print $0&#125;END&#123; print &quot;over&quot;&#125;]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sed学习笔记]]></title>
    <url>%2F2017%2F04%2F26%2Fsed%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[学习使用sedThis blog is used to record the usage of sed.sed 不会对原始文件产生影响，一行行读取，一行行匹配处理 正则表达式正则表达式,在sed里面要加两个slash ^匹配开头 /^#/ 匹配#开头的行，注释行 $匹配末尾 /^$/ 匹配空行 /./ 匹配一个字符 /../ 匹配两个字符 * 匹配前面0个或多个字符 剔除指定行，打印剩余行 sed -e ‘1,5d’ filename 删除1到5行，打印剩余行 sed -e ‘/^#/d’ filename 删除所有#开头的行，打印剩余行 sed -e ‘/leslie/p’ filename 删除所有包含leslie的行，打印剩余行 打印指定的行 sed -n -e ‘1,5p’ filename 打印1到5行 sed -n -e ‘/^#/p’ filename 打印所有#开头的行 sed -n -e ‘/leslie/p’ filename 打印所有包含leslie的行 sed -n -e ‘regureexpression1,regureexpression2’ filename 从匹配第一个正则表达式的第一行开始到匹配第二正则表达式的第一行 sed -e ‘=’ filename 打印行号 替换sed -e ‘s/word1/word2/g’ filenames表示替换，用word2替换word1g表示全局替换，没有g则只会对第一个出现word1的位置替换为word2 组合多条命令使用；分割多条命令 sed -n -e ‘1,5=;1,5p’ filename或者 sed -n -e ‘1,5=’ -e ‘1,5p’ filename 读取文本中的命令新建一个 (command).sed后缀名的文件在里面写入命令sed -n -f command.sed filename]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[handlebars]]></title>
    <url>%2F2017%2F04%2F18%2Fhandlebars%2F</url>
    <content type="text"><![CDATA[介绍Handlebars 作为一种模板引擎可以很好的实现前端html代码的模块化和复用 基本的用法参考这个链接 里面描述的很清楚这个是hbs的官网 也提供了参考的代码这个是博客中代码的地址 提供了参考的代码]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb]]></title>
    <url>%2F2017%2F04%2F16%2FMongodb%2F</url>
    <content type="text"><![CDATA[基本术语collection 对应了 tabledocument 对应了 rowfield 对应了 columndatabase，index，primary key都是一致的 Macos用homebrew安装， 运行 mongodb：两种方式方式1：打开mongodb的图形化界面，默认连接的数据目录的配置文件在/usr/local/etc/mongod.conf可以修改/data/db的路径 方式2： 命令行下，在shell中进入安装目录，我的mac：/Applications/MongoDB.app/Contents/Resources/Vendor/mongodb将这个目录配置到PATH环境变量里面配置数据目录 默认路径为/data/db,运行 sudo mongod(需要sudo，因为/data/db在根目录),这样就打开了mongodb，可以等待连接了或者在用户目录~下建立~/data/db,然后在启动mongod时：mongod -dapath ~/data/path 管理mongodb：两种方式 命令行运行mongo把上面的路径配置到PATH下面，运行mongo，进入mongo-shell管理界面可以用show dbs显示所有数据库use db_name使用某个数据库 1234# 查看collection中的所有记录db.collection.find()# 查看collection中的记录的数量db.collection.count() 用用MongoChef图形化界面进行管理运行mongo进入管理界面之后， Nodejs接口Nodejs下面有两种模块的接口支持对MongoDB的访问，mongodb的官网使用的是mongodb, 但是在实际生产环境使用Mongoose更多一点 mongodbmongodb的使用和在mongo的命令行从操作数据库比较类似用GraphQL和mongondb写过一些操作的例子，具体的语法可以参考官网的教程，或者github代码链接 Mongoose官网中文教程在nodejs下面可以用Mongoose模块进行mongodb管理schema，model，的概念，先定义一个schema对应了数据表(collection)的结构，然后用schema来创建一个一个model，使用model来具体操作数据(document),代码的组织可以参考这个中文教程一般会在一个独立的文件中新建一个schema以及model，然后将这个model export出来在需要用到的地方require这个model 保存数据document=new model（data）传入数据，创建一个新的document（一行数据）document.save（function（）{ }）调用save保存到数据库中，在回调函数中进行处理 更新数据,三个参数model.update(query，data，callback)1.query是匹配查找你想要更新哪一行数据(哪个document)2.data是你希望更新的数据1.set修改数据2.如果数据的数组，可以往里面push，pop一个element3.callback，是更新结束之后的回调函数 查询model.find(function(){})在回调函数中进行查询到的数据的处理 Pymongo在python下面可以通过pymongo模块进行mongodb的管理12345678910111213141516171819202122232425262728293031323334353637### 连接数据库client = pymongo.MongoClient(dbConfig[&apos;url&apos;], dbConfig[&apos;port&apos;])### 获取数据库db = client[&apos;quant&apos;] # same as &apos;db = client.quant&apos;### 获取Collectioncollection = db[&apos;tradeHistoryData&apos;]### 创建数据库db = client[&apos;quant&apos;] #如果数据库不存在会自己创建### 创建collectioncollection = db[&apos;tradeHistoryData&apos;] # collection 不存在则会被创建### 插入数据#### 单挑插入document1 = &#123;&#125; ### 创建数据，字典类型post_1 = collection.insert_one(document1).inserted_id ##其中.inserted_id将返回ObjectId对象#### 批量插入new_posts = [&#123;document1&#125;,&#123;document2&#125;]result = collection.insert_many(new_posts)### 获取单条数据(document)collection.find_one()### 获取所有documentlines = collection.find()lines = collection.find(&#123;&quot;author&quot;: &quot;Mike&quot;&#125;) ## 添加查找的条件collection.find(&#123;&quot;author&quot;: &quot;Mike&quot;&#125;).count ## 计数### 创建索引result = collection.create_index([(&apos;user_id&apos;, pymongo.ASCENDING)], unique=True)### 更新数据collection.update_one(&#123;&apos;x&apos;:4&#125;,&#123;&apos;$set&apos;:&#123;&apos;x&apos;:3&#125;&#125;) ##其中传入的第一个参数是你想要更新的数据，第二个是你想要更新的最新数据。其中$set部分是必要元素，如果没有会报出错误。除了$set外还有很多其它的比如$inc，对应着不同的功能### 删除数据collection.delete_one(&#123;&apos;x&apos;:3&#125;)### 关闭连接client.close() 数据库冷备份和还原参考这篇文章可以冷备份还原数据库；数据库中的指定表；指定表的指定字段]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django]]></title>
    <url>%2F2017%2F04%2F15%2FDjango%2F</url>
    <content type="text"><![CDATA[开发过程新建一个app之后，在settings.py的INSTALLED_APPS中需要注册一下这个APP 模板的配置在app的目录下面新建一个tempaltes的目录在base_dir/app_name/templates/app_name/xxx.html这个目录下面去写html然后在setting.py的TEMPLATES的’DIRS’加上，[os.path.join(BASE_DIR, app_name, ‘templates’),这样在app下面的views里面就可以render这个html了，然后在url里面做配置 静态文件的配置在app的目录下面新建一个static的目录在base_dir/app_name/static/app_name/目录下面去建imgaes，css,js等目录在下面摆放静态文件在settings.py按照模板配置 集成ajax主要是要注意：在用ajax提交表单的时候，需要在header中包含进csrf-token 集成reactjsDjango默认使用的templates比较适合用在静态页面(页面不太变化的场景下面)，所有的html中的数据都通过context传进去。Reactjs则比较适合用在动态页面中，一般会在项目的根目录下建立一个目录(front)专门用于储存reactjs的代码，同时将reactjs的code经过webpack之后的代码放入另一个目录中(webpack)。在Django的setting中static文件需要包含从这个webpack目录中去找。在Django的html文件中：1.需要包含reactjs渲染的主块 2.另外reactjs的代码express中是通过script包含进去的，在django中则是使用django webpack load去包含进去。 同时在webpack中可以配置生成的文件用hash之后的名字命名。这样在生产环境中可以避免浏览器缓存的问题，就是浏览器缓存了上一版本的js文件，用hash之后的文件名就会重新加载。webpack会生成一个json文件，里面包含了webpack前后的文件名的对应关系，这样在html中引入webpack之后的hash名的js文件的时候，就不需要没有刺激都修改引入的文件名了。比如：通过webpack-bundle-tracker会写入webpack-stats.json文件中 python manage.py runserver 来启动django 敲gulp 来生成babel和webpack的文件 webpack配置webpack-bundle-tracker来生成webpack-stats.json文件 部署部署到阿里云上，创建好数据库之后，启动，会报错说数据表不存在，python manager.py makemigrations 也会报同样的错有两种可能的解决方法： 数据迁移把开发环境中的数据都迁移到云端，mysql比较方便，用冷迁移就可以了备份数据: mysqldump -u root -p databaseName &gt; data.sql恢复: mysqldump -u root -p databaseName &lt; data.sql 当然mysql也支持数据的热备份，只需要在mysql中配置就可以了，本质上就是在A服务器上对数据库操作的sql语句会备份在一个log文件中，B服务器会监听这个log文件，实时更新到本地，并执行对应的sql语句 将各个APP下面的migration文件夹中的00x开头的记录django数据model变化的文件删除，然后重新migrate一下]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python_CookBook]]></title>
    <url>%2F2017%2F04%2F15%2FPython-CookBook%2F</url>
    <content type="text"><![CDATA[介绍最近在读cookbook， 记录一下阅读的心得总的感觉是书上介绍的很多用法都很精妙，有些方法也非常实用第一遍读下来还有很多高级的用法看得不是很明白，需要多写，多读源码在实践中去掌握这些写法和用法里面还是也很多的用法不熟悉，还需要好好努力！]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黄山游记]]></title>
    <url>%2F2017%2F03%2F06%2F%E9%BB%84%E5%B1%B1%E6%B8%B8%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[黄山游记记录一趟短途的出行 2017.2.26 - 2017.2.28 游历黄山 日落2017.2.27日 黄山 丹霞峰上观看落日 日出2017.2.28日 黄山 光明顶观看日出 始信峰2017.2.27 黄山 始信峰上观看奇峰怪石]]></content>
      <tags>
        <tag>游记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建个人博客]]></title>
    <url>%2F2017%2F03%2F05%2F%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98%E4%BF%AE%E6%94%B9%2F</url>
    <content type="text"><![CDATA[如何利用git搭建个人博客工具github+nodejs+hexo首先安装nvm，然后安装node然后安装hexo npm install -g hexo-cli 下面是一些hexo常用的命令:1234hexo clean #清除public文件夹下原有的构建hexo generate #重新构建hexo deploy #部署到git上hexo server #可以先不部署，在本地运行看看效果，再部署到git上面 定制自己的博客主题可以从git上下载自己的喜欢的主题：git的仓库地址可以自行在git上搜索在配置文件_config.xml中进行配置具体的配置过程可以参考一下简书的文章 语法支持markdown语法，根据选择的主题不同会有所差别可以插入图片，本地图片或云端图片，这里推荐使用储存在七牛云上的图片]]></content>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
</search>
